{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3166d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from scipy.linalg import block_diag, inv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import t, zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b48f13",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model Data Preprossing\n",
    "\n",
    "The users need to define the input information like brain regions of interest, which measurement to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4feb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input info\n",
    "ROI_name_list = ['Lamyg', 'Ramyg']\n",
    "#ROI_name_list = ['L_precentral_thickavg', 'R_precentral_thickavg']\n",
    "covar_dynamic_name_list = ['Age', 'Sex', 'ICV'] \n",
    "# recode sex info!\n",
    "covar_stable_name_list = ['C1', 'C2','C3', 'C4','APOE4'] \n",
    "meausure_time_list = ['sc', '12mo', '24mo']\n",
    "#n_measure_point = len(meausure_time_list)\n",
    "#n_timepoint = 3\n",
    "#slice_length = 3\n",
    "\n",
    "#genetic information\n",
    "# Input files\n",
    "# pycaret package ~\n",
    "Diagnosis = 'All'\n",
    "#path_name = '/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Subcortical_Volume'\n",
    "#pheno_file_name = '/ADNI1plus2_MCI_MDS.csv'\n",
    "\n",
    "#path_name = '/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Subcortical_Volume'\n",
    "#pheno_file_name = '/ADNI1plus2_Dementia_MDS.csv'\n",
    "\n",
    "path_name = '/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Subcortical_Volume_Total_Analysis/Total_MDS'\n",
    "pheno_file_name = '/ADNI1plus2_total_sorted_MDS_final.csv'\n",
    "\n",
    "pheno_merge_name = path_name + pheno_file_name\n",
    "\n",
    "grm_name = path_name + '/ADNI1plus2_GRM_total_sorted_final.csv'\n",
    "\n",
    "# output path name\n",
    "output_path_name = '/Users/aliceyang'\n",
    "file_type_name = '.csv'\n",
    "\n",
    "# the time point/spatial location ifo should not be coded in the subject ID!\n",
    "# cohort info\n",
    "subject_ID_name = 'SubjID'\n",
    "subject_ID_slice_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706a3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input info\n",
    "#ROI_name_list = ['Lhippo', 'Lhippo']\n",
    "#covar_dynamic_name_list = ['Age', 'Sex'] \n",
    "## recode sex info!\n",
    "#covar_stable_name_list = ['APOE4'] \n",
    "#meausure_time_list = ['sc', '12mo', '24mo']\n",
    "#n_measure_point = len(meausure_time_list)\n",
    "#n_timepoint = 3\n",
    "#slice_length = 3\n",
    "\n",
    "#genetic information\n",
    "#grm_path = None\n",
    "\n",
    "# Input files\n",
    "# pycaret package ~\n",
    "#path_name = '/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Subcortical_Volume'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_CN_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_MCI_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_Dementia_final_sort.csv'\n",
    "#pheno_merge_name = path_name + pheno_file_name\n",
    "#diagnosis_name = 'sAD' # diagnosis name may be extracted from file name\n",
    "\n",
    "# output path name\n",
    "#output_path_name = '/Users/aliceyang/result'\n",
    "#output_file_name = 'res'\n",
    "#file_type_name = '.csv'\n",
    "\n",
    "# the time point/spatial location ifo should not be coded in the subject ID!\n",
    "# cohort info\n",
    "##subject_ID_name = 'SubjID'\n",
    "#subject_ID_slice_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f72cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Subcortical_Volume_Total_Analysis/Total_MDS/ADNI1plus2_total_sorted_MDS_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(pheno_merge_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef5a69",
   "metadata": {},
   "source": [
    "# Read files and load column information to the corresponding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc251d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_info_from_file(filename, ROI_name_list, covar_dynamic_name_list, meausure_time_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    read input csv file containing all phenotype/genotype/covariates such as age and sex.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------\n",
    "    filename: str, csv filename.\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    df: dataframe of the csv file.\n",
    "    \n",
    "    n: total number of measurements for all subjects.\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(filename)\n",
    "    #print(ROI_name_list)\n",
    "    #print(covar_dynamic_name_list)\n",
    "    #print(meausure_time_list)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        print('file not found!')\n",
    "        sys.exit()\n",
    "    # n = df.shape[0]\n",
    "    \n",
    "    dict_roi_name = {}\n",
    "    for roi in ROI_name_list:\n",
    "        dict_roi_name[roi] = []\n",
    "        for tp in meausure_time_list:\n",
    "            col_name = roi + '_' + tp\n",
    "            if col_name not in df.columns:\n",
    "                print(col_name + 'does not exist!')\n",
    "                sys.exit()\n",
    "            dict_roi_name[roi].append(col_name)\n",
    "    #print(dict_roi_name)\n",
    "    \n",
    "    dict_dynamic_cov_name = {}\n",
    "    for cov in covar_dynamic_name_list:\n",
    "        dict_dynamic_cov_name[cov] = []\n",
    "        for tp in meausure_time_list:\n",
    "            col_name = cov + '_' + tp\n",
    "            if col_name not in df.columns:\n",
    "                print(col_name + 'does not exist!')\n",
    "                sys.exit()\n",
    "            dict_dynamic_cov_name[cov].append(col_name)\n",
    "    #print(dict_dynamic_cov_name)\n",
    "    return df, dict_roi_name, dict_dynamic_cov_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00447e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_matrix_T(measurement):\n",
    "    \"\"\"\n",
    "    \n",
    "    get mapping matrix to map cross sectional varibles (covariates/ranfom effect correlation) to longitudinal information.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------    \n",
    "    measurement: list, each entry represents the number of measurements for each subject.\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}. \n",
    "    \n",
    "    \"\"\"\n",
    "    T = np.expand_dims(np.repeat(1, measurement[0]), axis=1)\n",
    "    for i in range(1, np.shape(measurement)[0]): \n",
    "        T_pre = T\n",
    "        T = block_diag(T_pre, np.expand_dims(np.repeat(1, measurement[i]), axis = 1))\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ac86832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_longit_mapping_function(matrix_T, correlation_matrix):\n",
    "    \"\"\"\n",
    "    \n",
    "    map function for cross-sectional covariate information or random effect correlation information to longitudinal.\n",
    "    T=blkdiag{1_n1 , ···,1_nm} \n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------  \n",
    "    correlation_matrix: numpy array N x N, the correlation matrix specified for the normal distribution. N number of subjects\n",
    "    \n",
    "    matrix_T: numpy array ? x ?, the function to map cross sectional to longitudinal.\n",
    "    \n",
    "\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    longit_correlation: longitudinal correlation matrix mapped from cross sectional \n",
    "    \n",
    "    \"\"\"\n",
    "    # use @\n",
    "    T_correlation = np.matmul(matrix_T, correlation_matrix)\n",
    "    T_correlation_T_transpose = np.matmul(T_correlation, np.transpose(matrix_T))\n",
    "    N_total_measurement = np.shape(T_correlation_T_transpose)[0]\n",
    "    longit_correlation = T_correlation_T_transpose\n",
    "    #longit_correlation_1d = T_correlation_T_transpose.flatten('F')\n",
    "    return longit_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65d37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grm_info_from_file(n_subject, filename):\n",
    "    \"\"\"\n",
    "       ! optional \n",
    "    \n",
    "    \"\"\"\n",
    "    if filename == None:\n",
    "        grm = np.identity(n_subject)\n",
    "    else:\n",
    "        grm = pd.read_csv(filename, header = None)\n",
    "    return grm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab212d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_basic_subject_measurement_info_from_file(n_subject, filename, ROI_name_list, ROI_index = None):\n",
    "    # how to caluculate the number of measurements for each subject?\n",
    "    if filename == None:\n",
    "        measurement = df.columns[df.columns.str.startswith(ROI_name_list[ROI_index])]\n",
    "    else:\n",
    "        measurement = pd.read_csv(filename, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3f921a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubjID</th>\n",
       "      <th>DX_sc</th>\n",
       "      <th>DX_12mo</th>\n",
       "      <th>DX_24mo</th>\n",
       "      <th>ICV_sc</th>\n",
       "      <th>ICV_12mo</th>\n",
       "      <th>ICV_24mo</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>Age_sc</th>\n",
       "      <th>Age_12mo</th>\n",
       "      <th>...</th>\n",
       "      <th>Lput_24mo</th>\n",
       "      <th>Rput_24mo</th>\n",
       "      <th>Lpal_24mo</th>\n",
       "      <th>Rpal_24mo</th>\n",
       "      <th>Lhippo_24mo</th>\n",
       "      <th>Rhippo_24mo</th>\n",
       "      <th>Lamyg_24mo</th>\n",
       "      <th>Ramyg_24mo</th>\n",
       "      <th>Laccumb_24mo</th>\n",
       "      <th>Raccumb_24mo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>1650465.826</td>\n",
       "      <td>1652724.735</td>\n",
       "      <td>1652220.105</td>\n",
       "      <td>1</td>\n",
       "      <td>84.9</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5129.8</td>\n",
       "      <td>4798.0</td>\n",
       "      <td>1493.6</td>\n",
       "      <td>1547.6</td>\n",
       "      <td>3328.2</td>\n",
       "      <td>3406.0</td>\n",
       "      <td>1151.9</td>\n",
       "      <td>1488.8</td>\n",
       "      <td>253.9</td>\n",
       "      <td>315.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_0559</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>1717852.074</td>\n",
       "      <td>1722711.640</td>\n",
       "      <td>1714273.951</td>\n",
       "      <td>1</td>\n",
       "      <td>79.4</td>\n",
       "      <td>80.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4517.2</td>\n",
       "      <td>4637.4</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>1565.2</td>\n",
       "      <td>3561.2</td>\n",
       "      <td>3632.5</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>1778.1</td>\n",
       "      <td>430.2</td>\n",
       "      <td>395.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_0685</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>1538868.691</td>\n",
       "      <td>1524683.341</td>\n",
       "      <td>1528160.768</td>\n",
       "      <td>0</td>\n",
       "      <td>89.7</td>\n",
       "      <td>90.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4332.4</td>\n",
       "      <td>5457.0</td>\n",
       "      <td>1543.5</td>\n",
       "      <td>1024.4</td>\n",
       "      <td>3572.8</td>\n",
       "      <td>3335.0</td>\n",
       "      <td>1162.2</td>\n",
       "      <td>1096.4</td>\n",
       "      <td>352.0</td>\n",
       "      <td>219.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_S_1261</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>1500525.222</td>\n",
       "      <td>1487317.186</td>\n",
       "      <td>1494543.089</td>\n",
       "      <td>0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>72.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4087.7</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>1411.8</td>\n",
       "      <td>1608.8</td>\n",
       "      <td>2769.7</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>778.0</td>\n",
       "      <td>1070.7</td>\n",
       "      <td>160.8</td>\n",
       "      <td>259.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002_S_4213</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN</td>\n",
       "      <td>1498839.046</td>\n",
       "      <td>1499090.796</td>\n",
       "      <td>1498901.597</td>\n",
       "      <td>0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>79.1</td>\n",
       "      <td>...</td>\n",
       "      <td>3589.5</td>\n",
       "      <td>3962.2</td>\n",
       "      <td>912.3</td>\n",
       "      <td>1083.8</td>\n",
       "      <td>4108.6</td>\n",
       "      <td>3703.1</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>1398.5</td>\n",
       "      <td>475.3</td>\n",
       "      <td>594.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>136_S_0426</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1995233.554</td>\n",
       "      <td>1998866.206</td>\n",
       "      <td>2019543.872</td>\n",
       "      <td>1</td>\n",
       "      <td>79.7</td>\n",
       "      <td>80.7</td>\n",
       "      <td>...</td>\n",
       "      <td>6183.6</td>\n",
       "      <td>5859.4</td>\n",
       "      <td>1644.1</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>3068.8</td>\n",
       "      <td>3460.6</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1369.5</td>\n",
       "      <td>403.6</td>\n",
       "      <td>352.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>137_S_0366</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1290328.655</td>\n",
       "      <td>1296791.257</td>\n",
       "      <td>1293193.817</td>\n",
       "      <td>0</td>\n",
       "      <td>56.7</td>\n",
       "      <td>57.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3449.1</td>\n",
       "      <td>3535.3</td>\n",
       "      <td>1183.5</td>\n",
       "      <td>1211.8</td>\n",
       "      <td>2625.1</td>\n",
       "      <td>3078.1</td>\n",
       "      <td>749.1</td>\n",
       "      <td>1005.3</td>\n",
       "      <td>269.7</td>\n",
       "      <td>271.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>137_S_4211</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1350125.957</td>\n",
       "      <td>1332750.512</td>\n",
       "      <td>1333434.641</td>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>82.1</td>\n",
       "      <td>...</td>\n",
       "      <td>3820.8</td>\n",
       "      <td>3197.3</td>\n",
       "      <td>1284.2</td>\n",
       "      <td>1242.6</td>\n",
       "      <td>2339.4</td>\n",
       "      <td>1953.6</td>\n",
       "      <td>812.5</td>\n",
       "      <td>752.7</td>\n",
       "      <td>316.1</td>\n",
       "      <td>381.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>137_S_4258</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1856051.163</td>\n",
       "      <td>1821546.976</td>\n",
       "      <td>1742281.420</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4179.3</td>\n",
       "      <td>4436.2</td>\n",
       "      <td>1404.1</td>\n",
       "      <td>2014.5</td>\n",
       "      <td>4081.0</td>\n",
       "      <td>3718.9</td>\n",
       "      <td>1072.3</td>\n",
       "      <td>1124.5</td>\n",
       "      <td>119.7</td>\n",
       "      <td>272.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>153_S_4172</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1694922.913</td>\n",
       "      <td>1692415.099</td>\n",
       "      <td>1693798.595</td>\n",
       "      <td>0</td>\n",
       "      <td>75.9</td>\n",
       "      <td>76.9</td>\n",
       "      <td>...</td>\n",
       "      <td>3656.7</td>\n",
       "      <td>3683.2</td>\n",
       "      <td>1628.5</td>\n",
       "      <td>1513.8</td>\n",
       "      <td>3056.0</td>\n",
       "      <td>3701.8</td>\n",
       "      <td>987.8</td>\n",
       "      <td>1177.2</td>\n",
       "      <td>243.2</td>\n",
       "      <td>274.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SubjID     DX_sc   DX_12mo   DX_24mo       ICV_sc     ICV_12mo  \\\n",
       "0    002_S_0295        CN        CN        CN  1650465.826  1652724.735   \n",
       "1    002_S_0559        CN        CN        CN  1717852.074  1722711.640   \n",
       "2    002_S_0685        CN        CN        CN  1538868.691  1524683.341   \n",
       "3    002_S_1261        CN        CN        CN  1500525.222  1487317.186   \n",
       "4    002_S_4213        CN        CN        CN  1498839.046  1499090.796   \n",
       "..          ...       ...       ...       ...          ...          ...   \n",
       "700  136_S_0426  Dementia  Dementia  Dementia  1995233.554  1998866.206   \n",
       "701  137_S_0366  Dementia  Dementia  Dementia  1290328.655  1296791.257   \n",
       "702  137_S_4211  Dementia  Dementia  Dementia  1350125.957  1332750.512   \n",
       "703  137_S_4258  Dementia  Dementia  Dementia  1856051.163  1821546.976   \n",
       "704  153_S_4172  Dementia  Dementia  Dementia  1694922.913  1692415.099   \n",
       "\n",
       "        ICV_24mo  APOE4  Age_sc  Age_12mo  ...  Lput_24mo  Rput_24mo  \\\n",
       "0    1652220.105      1    84.9      86.0  ...     5129.8     4798.0   \n",
       "1    1714273.951      1    79.4      80.5  ...     4517.2     4637.4   \n",
       "2    1528160.768      0    89.7      90.8  ...     4332.4     5457.0   \n",
       "3    1494543.089      0    71.3      72.5  ...     4087.7     3887.0   \n",
       "4    1498901.597      0    78.1      79.1  ...     3589.5     3962.2   \n",
       "..           ...    ...     ...       ...  ...        ...        ...   \n",
       "700  2019543.872      1    79.7      80.7  ...     6183.6     5859.4   \n",
       "701  1293193.817      0    56.7      57.8  ...     3449.1     3535.3   \n",
       "702  1333434.641      0    81.0      82.1  ...     3820.8     3197.3   \n",
       "703  1742281.420      1    76.0      77.0  ...     4179.3     4436.2   \n",
       "704  1693798.595      0    75.9      76.9  ...     3656.7     3683.2   \n",
       "\n",
       "     Lpal_24mo  Rpal_24mo Lhippo_24mo Rhippo_24mo  Lamyg_24mo  Ramyg_24mo  \\\n",
       "0       1493.6     1547.6      3328.2      3406.0      1151.9      1488.8   \n",
       "1       1288.0     1565.2      3561.2      3632.5      1485.0      1778.1   \n",
       "2       1543.5     1024.4      3572.8      3335.0      1162.2      1096.4   \n",
       "3       1411.8     1608.8      2769.7      2936.0       778.0      1070.7   \n",
       "4        912.3     1083.8      4108.6      3703.1      1235.0      1398.5   \n",
       "..         ...        ...         ...         ...         ...         ...   \n",
       "700     1644.1     1894.0      3068.8      3460.6      1093.0      1369.5   \n",
       "701     1183.5     1211.8      2625.1      3078.1       749.1      1005.3   \n",
       "702     1284.2     1242.6      2339.4      1953.6       812.5       752.7   \n",
       "703     1404.1     2014.5      4081.0      3718.9      1072.3      1124.5   \n",
       "704     1628.5     1513.8      3056.0      3701.8       987.8      1177.2   \n",
       "\n",
       "     Laccumb_24mo  Raccumb_24mo  \n",
       "0           253.9         315.7  \n",
       "1           430.2         395.2  \n",
       "2           352.0         219.8  \n",
       "3           160.8         259.9  \n",
       "4           475.3         594.4  \n",
       "..            ...           ...  \n",
       "700         403.6         352.4  \n",
       "701         269.7         271.7  \n",
       "702         316.1         381.4  \n",
       "703         119.7         272.2  \n",
       "704         243.2         274.3  \n",
       "\n",
       "[705 rows x 68 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(pheno_merge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334293f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain measures (left entorhinal thickavg):\n",
    "#df_pheno_merge, dict_roi_name, dict_dynamic_cov_name = load_info_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "#print(dict_roi_name)\n",
    "#print(dict_dynamic_cov_name)\n",
    "\n",
    "# Calcuate the number of participants\n",
    "#n_subject = len(df_pheno_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcbba858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grm = load_grm_info_from_file(n_subject, filename = grm_name)\n",
    "#grm = np.array(grm)\n",
    "df_pheno_merge, dict_roi_name, dict_dynamic_cov_name = load_info_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c7b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pheno_measurement_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list):\n",
    "# load phenotype information and get average measures as the longitudinal measure to train/test\n",
    "    #df_pheno_merge, dict_roi_name, dict_dynamic_cov_name = load_info_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "    total_measure = []\n",
    "    longit_measure = []\n",
    "    for roi, longit_roi in dict_roi_name.items():\n",
    "        total_measure.append(df_pheno_merge[longit_roi])\n",
    "\n",
    "    longit_measure = np.mean(total_measure, axis = 0)\n",
    "    longit_measure = np.expand_dims(longit_measure.flatten('C'), axis = 1)\n",
    "    #longit_measure_df = pd.DataFrame(longit_measure)\n",
    "    #longit_measure_df.to_csv('pheno_cMCI_precentral_TH.csv')\n",
    "    #print(np.shape(longit_measure))\n",
    "    \n",
    "    # the number of max time points\n",
    "    n_timepoint = len(dict_roi_name)\n",
    "    time_intervals = list(range(n_timepoint))\n",
    "    count_measure = []\n",
    "    for roi, col_name in dict_roi_name.items():\n",
    "        # left and right\n",
    "        #print(roi)\n",
    "        #print(col_name)\n",
    "        count = df_pheno_merge[col_name].notna().sum(axis = 1)\n",
    "        count_measure.append(count)\n",
    "    measurement = np.min(count_measure, axis = 0)\n",
    "    #print(measurement)\n",
    "    return measurement, longit_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e861746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load phenotype information and get average measures as the longitudinal measure to train/test\n",
    "#total_measure = []\n",
    "#longit_measure = []\n",
    "#for roi, longit_roi in dict_roi_name.items():\n",
    "    #total_measure.append(df_pheno_merge[longit_roi])\n",
    "\n",
    "#longit_measure = np.mean(total_measure, axis = 0)\n",
    "#longit_measure = np.expand_dims(longit_measure.flatten('C'), axis = 1)\n",
    "#print(np.shape(longit_measure))\n",
    "measurement, longit_measure = load_pheno_measurement_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de38315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for cross validation group info\n",
    "def load_groups_from_file(measurement):\n",
    "    k = 0\n",
    "    groups = []\n",
    "    for cnt_meas in measurement:\n",
    "        for i in range(cnt_meas):\n",
    "            groups.append(k)\n",
    "        k = k + 1\n",
    "    groups = np.array(groups)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64080190",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = load_groups_from_file(measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d06bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of max time points\n",
    "#n_timepoint = len(dict_roi_name)\n",
    "\n",
    "#time_intervals = list(range(n_timepoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c14cb1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of measurements per each subject\n",
    "#subset_df = df[dict_roi_name[0]]\n",
    "\n",
    "#count_measure = []\n",
    "#for roi, col_name in dict_roi_name.items():\n",
    "    # left and right\n",
    "    #print(roi)\n",
    "    #print(col_name)\n",
    "    #count = df_pheno_merge[col_name].notna().sum(axis = 1)\n",
    "    #count_measure.append(count)\n",
    "#measurement = np.min(count_measure, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041abd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mapping matrix T to map from cross sectional to longitudinal\n",
    "T = mapping_matrix_T(measurement)\n",
    "\n",
    "# do we need group info for cross validation from meausrement? genetic correlated?\n",
    "#k = 0\n",
    "#groups = []\n",
    "#for cnt_meas in measurement:\n",
    "    #for i in range(cnt_meas):\n",
    "        #groups.append(k)\n",
    "    #k = k + 1\n",
    "#groups = np.array(groups)\n",
    "#print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a0654d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.850607e-01  7.943848e-03  8.126090e-04 ...  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00]\n",
      " [ 7.943848e-03  9.976281e-01  5.052495e-03 ...  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00]\n",
      " [ 8.126090e-04  5.052495e-03  1.001683e+00 ...  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00]\n",
      " ...\n",
      " [ 0.000000e+00  0.000000e+00  0.000000e+00 ...  9.871954e-01\n",
      "   7.460000e-05 -4.621029e-03]\n",
      " [ 0.000000e+00  0.000000e+00  0.000000e+00 ...  7.460000e-05\n",
      "   9.892845e-01 -3.635682e-03]\n",
      " [ 0.000000e+00  0.000000e+00  0.000000e+00 ... -4.621029e-03\n",
      "  -3.635682e-03  1.003025e+00]]\n"
     ]
    }
   ],
   "source": [
    "n_subject = len(df_pheno_merge)\n",
    "grm = load_grm_info_from_file(n_subject, grm_name)\n",
    "grm = np.array(grm)\n",
    "print(grm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363f0b",
   "metadata": {},
   "source": [
    "# Generate random effect correlation matrix information and extract covariance information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "305dc286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_effect_info_from_file(grm, measurement, subject_level, T):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    get the random effect covariance matrix for each of the ranfom effect (genetic relationship/temporal or spatial effect/site effect/measurement error)\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    grm: numpy array, input Genetic Relationship Matrix (GRM) for subject-subject correlation.\n",
    "    \n",
    "    measurement: list, each entry represents the number of measurements for any subject.\n",
    "    \n",
    "    subject_level: numpy array, \n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    Return(s)\n",
    "    ----------  \n",
    "    longit_grm: numpy array, one-dimensional array for longitudinal Genetic Relationship Matrix (GRM).\n",
    "    \n",
    "    longit_C1: numpy array,  one-dimensional array for longitudinal correlation matrix corresponding to time intervals between two closest measurements. \n",
    "    \n",
    "    longit_C2: numpy array,  one-dimensional array for longitudinal correlation matrix corresponding to time intervals between two closest measurements.\n",
    "    \n",
    "    longit_block_diag_matrix: numpy array,  one-dimensional array for longitudinal block diagnoal matrix \n",
    "    \n",
    "    longit_error: numpy array, one-dimensional array for indepedent measurement erors.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Create longitudinal GRM 1d info\n",
    "    grm_arr = np.array(grm)\n",
    "    longit_grm = cross_sectional_longit_mapping_function(T, grm_arr)\n",
    "    \n",
    "# Create covariate matrix input for temporal or spatialy correlated random effect using time intervals or distance functions\n",
    "# hypothetically  \n",
    "    if measurement is not None:# Create T always 3 time points\n",
    "        longit_C1 = np.zeros((measurement[0], measurement[0]))\n",
    "        longit_C2 = np.zeros((measurement[0], measurement[0]))\n",
    "        for i in range(measurement[0]):\n",
    "            for j in range(measurement[0]):\n",
    "                    if abs(i - j) == 1:\n",
    "                        longit_C1[i, j] = 1 # rho**abs(i - j) \n",
    "                    if abs(i - j) == 2:\n",
    "                        longit_C2[i, j] = 1 # rho**abs(i - j)\n",
    "    \n",
    "        for s in range(1, len(measurement)):\n",
    "            longit_C1_temp = np.zeros((measurement[s], measurement[s]))\n",
    "            longit_C2_temp = np.zeros((measurement[s], measurement[s]))\n",
    "            for i in range(measurement[s]):\n",
    "                for j in range(measurement[s]):\n",
    "                    if abs(i - j) == 1:\n",
    "                        longit_C1_temp[i, j] = 1 # rho**abs(i - j) \n",
    "                    if abs(i - j) == 2:\n",
    "                        longit_C2_temp[i, j] = 1 # rho**abs(i - j)\n",
    "            # C1 definition\n",
    "            longit_C1_pre = longit_C1\n",
    "            longit_C1 = block_diag(longit_C1_pre, longit_C1_temp)\n",
    "            # C2 definition\n",
    "            longit_C2_pre = longit_C2\n",
    "            longit_C2 = block_diag(longit_C2_pre, longit_C2_temp)\n",
    "        \n",
    "# Create longitudinal site 1d info using subject level\n",
    "    subject_level_dummy = pd.get_dummies(subject_level)\n",
    "    subject_level_cnt = subject_level_dummy.sum(axis=0)\n",
    "    block_diag_matrix = np.identity(subject_level_cnt[0])\n",
    "    for i in range(1, np.shape(subject_level_cnt)[0]):\n",
    "            block_diag_matrix_prev = block_diag_matrix\n",
    "            block_diag_matrix = block_diag(block_diag_matrix_prev, np.identity(subject_level_cnt[i]))\n",
    "    longit_block_diag_matrix = cross_sectional_longit_mapping_function(T, block_diag_matrix)\n",
    "    \n",
    "# create measurement error 1d info\n",
    "    longit_error = np.identity(np.sum(measurement))\n",
    "    return longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d26d2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of subjects in this study\n",
    "#len(df_pheno_merge)\n",
    "#sum(measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c9a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covar_info_from_file(df, covar_stable_name_list, dict_dynamic_cov_name, measurement, T):\n",
    "    \n",
    "    intercept = np.expand_dims(np.ones(np.sum(measurement)), axis=1)\n",
    "    if covar_stable_name_list is not None:\n",
    "        covar_stable_matrix = np.zeros((len(measurement),len(covar_stable_name_list)))\n",
    "        i = 0\n",
    "        for cov in covar_stable_name_list:\n",
    "            cov_value = df[cov]\n",
    "            #print(cov)\n",
    "            covar_stable_matrix[:, i] = cov_value\n",
    "            i = i + 1\n",
    "        #print(covar_stable_matrix)\n",
    "        covar_stable_matrix_df = pd.DataFrame(covar_stable_matrix)\n",
    "        covar_stable_matrix_df.to_csv('geno_cMCI_precentral_TH.csv')\n",
    "        \n",
    "        covar_stable_matrix_longit = np.matmul(T, covar_stable_matrix)\n",
    "        if dict_dynamic_cov_name is None:\n",
    "            covar_stable_matrix_longit = np.concatenate((intercept,covar_stable_matrix_longit), axis = 1) \n",
    "            return covar_stable_matrix_longit\n",
    "    \n",
    "    if dict_dynamic_cov_name is not None:\n",
    "        covar_dynamic_matrix_longit = np.zeros((sum(measurement), len(dict_dynamic_cov_name)))\n",
    "        j = 0\n",
    "        for cov in dict_dynamic_cov_name.keys():\n",
    "            #print(cov)\n",
    "            cov_longit_value = []\n",
    "            for cov_longit in dict_dynamic_cov_name[cov]:\n",
    "                cov_longit_value.append(df[cov_longit])\n",
    "            # bug?\n",
    "            cov_longit_value = np.array(cov_longit_value).flatten('F')\n",
    "            #print(cov_longit_value)\n",
    "            covar_dynamic_matrix_longit[:, j] = cov_longit_value\n",
    "            j = j + 1\n",
    "        if covar_stable_name_list is None:\n",
    "            covar_dynamic_matrix_longit = np.concatenate((intercept,covar_dynamic_matrix_longit), axis = 1)\n",
    "            return covar_dynamic_matrix_longit\n",
    "        \n",
    "        covar_matrix_longit = np.concatenate((intercept, covar_dynamic_matrix_longit, covar_stable_matrix_longit), axis = 1)\n",
    "        return covar_matrix_longit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c68682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subject level to represent scannner/location specific effects\n",
    "SubjID = df_pheno_merge[subject_ID_name]\n",
    "subject_level = [ID[:subject_ID_slice_length] for ID in SubjID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14207c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# load ranfom effect information\n",
    "longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "print(longit_C1)\n",
    "#print(longit_C2)\n",
    "#print(longit_block_diag_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9287d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': ['Age_sc', 'Age_12mo', 'Age_24mo'], 'Sex': ['Sex_sc', 'Sex_12mo', 'Sex_24mo'], 'ICV': ['ICV_sc', 'ICV_12mo', 'ICV_24mo']}\n",
      "['C1', 'C2', 'C3', 'C4', 'APOE4']\n"
     ]
    }
   ],
   "source": [
    "print(dict_dynamic_cov_name)\n",
    "print(covar_stable_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f078117",
   "metadata": {},
   "outputs": [],
   "source": [
    "longit_X = load_covar_info_from_file(df_pheno_merge, covar_stable_name_list, dict_dynamic_cov_name, measurement, T)\n",
    "#longit_X_df = pd.DataFrame(longit_X)\n",
    "#longit_X_df.to_csv('cov_cMCI_precentral_TH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23fc0b",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model parameter estimation: 2-step method\n",
    "step 1: project out covariance matrix X including age, sex, SNP encoding.\n",
    "\n",
    "step 2: run support vector regression to estimate random effect components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cd20589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step - Create projection matrix to regress out covariance matrix X\n",
    "\n",
    "def project_covariate_func(X):\n",
    "    \"\"\"\n",
    "    \n",
    "    get the covariates matrix when the users provide the name of coavariates needed.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    X: np.array, longittudinal covariate matrix inluding intercept.\n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    P: Projection matrix, so that X is projected to 0 matrix through P.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    XX = np.dot(np.transpose(X), X)\n",
    "    Z = np.dot(X, inv(XX))\n",
    "    P = np.diag(np.ones(X.shape[0])) - np.dot(Z, np.transpose(X))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3189c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd step - main autoregressive linear mixed model estimation function\n",
    "\n",
    "def ARLMM(Y, X, grm, measurement, subject_level, T):\n",
    "    \"\"\"\n",
    "    \n",
    "    get the estimated parameters (betas/random effect variances) as the 2nd step of ARLMM.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    Y: numpy array, one-dimensional longtidinal measures ordered by the time or spatial points.\n",
    "\n",
    "    X: numpy array, longitudinal covariate matrix inluding intercept.\n",
    "    \n",
    "    grm: numpy array, cross sectional genetic relationship matrix.\n",
    "    \n",
    "    measurement: list, each entry represents the number of measurements for any subject.\n",
    "    \n",
    "    subject_level: list, ...\n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    rho_est: numeric value, correlation parameter for temporal/spatial correlation matrix.\n",
    "    \n",
    "    var_g_est: numeric value, variance of genetic effect.\n",
    "    \n",
    "    var_t_est: numeric value, variance of imaging site effect.\n",
    "    \n",
    "    var_c_est: numeric value, variance of temporal/spatial correlated effect.\n",
    "    \n",
    "    var_e_est: numeric value, variance of meausurement error.\n",
    "    \n",
    "    h2_est: numeric value, proportion of genetic effect over total effect.\n",
    "    \n",
    "    beta_est: numpy array, beta values for covariance.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    P = project_covariate_func(X)\n",
    "    \n",
    "    longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "\n",
    "    project_longit_grm_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_grm),[-1, 1], order = 'F')\n",
    "    project_longit_C1_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C1), [-1, 1], order = 'F')\n",
    "    project_longit_C2_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C2), [-1, 1], order = 'F')\n",
    "    project_longit_block_diag_matrix_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_block_diag_matrix), [-1, 1], order = 'F')\n",
    "    project_longit_error_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_error).flatten('F'), [-1, 1], order = 'F')\n",
    "    Y_Y_transpose_1d = np.reshape(np.dot(Y, np.transpose(Y)), [-1, 1], order = 'F')\n",
    "    \n",
    "    project_longit_X_1d = np.concatenate((project_longit_grm_1d, project_longit_C1_1d, project_longit_C2_1d, project_longit_block_diag_matrix_1d, project_longit_error_1d), axis = 1)\n",
    "    #clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'huber', fit_intercept= False)\n",
    "    clf = SGDRegressor(penalty = 'l2', tol = 1e-3, loss = 'squared_error', fit_intercept= False, epsilon=3000, early_stopping=True)\n",
    "    clf.fit(project_longit_X_1d, Y_Y_transpose_1d.ravel())\n",
    "    #clf = MLPRegressor(random_state=1, early_stopping=True).fit(project_longit_X_1d, Y_Y_transpose_1d)\n",
    "    #clf.fit(project_longit_X_1d, Y_Y_transpose_1d)  \n",
    "    #n_layers = clf.n_layers_\n",
    "    #coefs = clf.coefs_\n",
    "    #print(n_layers)\n",
    "    #print(coefs)\n",
    "    \n",
    "    #from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "    #clf = PassiveAggressiveRegressor(max_iter=10000, fit_intercept = False, random_state = 0, tol = 1e-3)\n",
    "    #clf.fit(project_longit_X_1d, Y_Y_transpose_1d)\n",
    "    \n",
    "    #print(clf.score(project_longit_X_1d, Y_Y_transpose_1d))\n",
    "#################3    \n",
    "    sigma = clf.coef_\n",
    "    rho_est = max(0.00000000001, min(sigma[2]/sigma[1], 1))\n",
    "    var_t_est = max(0.5*sigma[1]/rho_est + 0.5*sigma[2]/(rho_est**2), 0.00000000001)\n",
    "    var_g_est = max(sigma[0], 0.00000000001)\n",
    "    var_e_est = max(sigma[3] - var_t_est, 0.00000000001)\n",
    "    var_c_est = max(sigma[4], 0.00000000001)\n",
    "    h2_est = var_g_est/(var_t_est + var_g_est + var_c_est + var_e_est)\n",
    "    \n",
    "    t_error_cov = var_t_est*(np.diag(np.ones(T.shape[0])) + rho_est*longit_C1 + rho_est**2*longit_C2)\n",
    "    error_cov = var_e_est*np.diag(np.ones(T.shape[0]))\n",
    "    genetic_cov = var_g_est*longit_grm\n",
    "    correlation_cov = var_c_est*longit_block_diag_matrix\n",
    "    total_V = t_error_cov + error_cov + genetic_cov + correlation_cov\n",
    "    \n",
    "    Y_new = np.dot(inv(total_V), Y)\n",
    "    repsonse_new = np.dot(np.transpose(X), Y_new)\n",
    "    X_new = np.dot(inv(total_V), X)\n",
    "    predictor_new = np.dot(np.transpose(X), X_new)\n",
    "    beta_est = np.dot(inv(predictor_new), repsonse_new)\n",
    "    \n",
    "    return rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcfecc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.78767279e+02]\n",
      " [ 9.58698042e-01]\n",
      " [-4.67668668e+01]\n",
      " [ 3.87534517e-04]\n",
      " [ 4.80793910e+03]\n",
      " [-2.09885373e+03]\n",
      " [ 2.62820964e+02]\n",
      " [-1.04692133e+03]\n",
      " [-7.77525619e+01]]\n",
      "0.34034295235048573\n"
     ]
    }
   ],
   "source": [
    "# run ARLMM main function once and generate best estimates for the whole dataset\n",
    "rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est = ARLMM(longit_measure, longit_X, grm, measurement, subject_level, T)\n",
    "print(beta_est)\n",
    "print(h2_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94eb389",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model (converter group) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "883c8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARLMM_late_converter(Y, X, grm, measurement, subject_level, T, stable_parameter):\n",
    "    # notice for late converter group the correlation matrix used are the same as stable group \n",
    "    # but stable parameter is known\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    get the estimated parameters (betas/random effect variances) as the 2nd step of ARLMM.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    Y: numpy array, one-dimensional longtidinal measures ordered by the time or spatial points.\n",
    "\n",
    "    X: numpy array, longitudinal covariate matrix inluding intercept.\n",
    "    \n",
    "    grm: numpy array, cross sectional genetic relationship matrix.\n",
    "    \n",
    "    measurement: list, each entry represents the number of measurements for any subject.\n",
    "    \n",
    "    subject_level: list, ...\n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    stable_parameter: numeric, \n",
    "    \n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    rho_est: numeric value, correlation parameter for temporal/spatial correlation matrix (converter).\n",
    "    \n",
    "    var_g_est: numeric value, variance of genetic effect.\n",
    "    \n",
    "    var_t_est: numeric value, variance of imaging site effect.\n",
    "    \n",
    "    var_c_est: numeric value, variance of temporal/spatial correlated effect.\n",
    "    \n",
    "    var_e_est: numeric value, variance of meausurement error.\n",
    "    \n",
    "    h2_est: numeric value, proportion of genetic effect over total effect.\n",
    "    \n",
    "    beta_est: numpy array, beta values for covariance.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    P = project_covariate_func(X)\n",
    "    \n",
    "    longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "    \n",
    "    project_longit_grm_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_grm),[-1, 1], order = 'F')\n",
    "    project_longit_C1_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C1), [-1, 1], order = 'F')\n",
    "    project_longit_C2_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C2), [-1, 1], order = 'F')\n",
    "    project_longit_block_diag_matrix_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_block_diag_matrix), [-1, 1], order = 'F')\n",
    "    project_longit_error_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_error).flatten('F'), [-1, 1], order = 'F')\n",
    "    Y_Y_transpose_1d = np.reshape(np.dot(Y, np.transpose(Y)), [-1, 1], order = 'F')\n",
    "    \n",
    "    project_longit_X_1d = np.concatenate((project_longit_grm_1d, project_longit_C1_1d, project_longit_C2_1d, project_longit_block_diag_matrix_1d, project_longit_error_1d), axis = 1)\n",
    "    #clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'huber', fit_intercept= False)\n",
    "    clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'squared_epsilon_insensitive', fit_intercept = False, early_stopping = True)\n",
    "    clf.fit(project_longit_X_1d, Y_Y_transpose_1d.ravel())\n",
    "    \n",
    "    sigma = clf.coef_\n",
    "    rho_est = max(0.00000000001, min(sigma[2]/sigma[1], 1)) \n",
    "    var_t_est = max(0.5*sigma[1]/stable_parameter + 0.5*sigma[2]/(rho_est*stable_parameter), 0.00000000001) \n",
    "    var_g_est = max(sigma[0], 0.00000000001) \n",
    "    var_e_est = max(sigma[3] - var_t_est, 0.00000000001)\n",
    "    var_c_est = max(sigma[4], 0.00000000001) \n",
    "    h2_est = var_g_est/(var_t_est + var_g_est + var_c_est + var_e_est)\n",
    "    \n",
    "    t_error_cov = var_t_est*(np.diag(np.ones(T.shape[0])) + stable_parameter*longit_C1 + rho_est*stable_parameter*longit_C2)\n",
    "    error_cov = var_e_est*np.diag(np.ones(T.shape[0]))\n",
    "    genetic_cov = var_g_est*longit_grm\n",
    "    correlation_cov = var_c_est*longit_block_diag_matrix\n",
    "    total_V = t_error_cov + error_cov + genetic_cov + correlation_cov\n",
    "    \n",
    "    Y_new = np.dot(inv(total_V), Y)\n",
    "    repsonse_new = np.dot(np.transpose(X), Y_new)\n",
    "    X_new = np.dot(inv(total_V), X)\n",
    "    predictor_new = np.dot(np.transpose(X), X_new)\n",
    "    beta_est = np.dot(inv(predictor_new), repsonse_new)\n",
    "    \n",
    "    return rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6ac4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARLMM_total_converter(Y, X, grm, measurement, subject_level, beta_late_convert):\n",
    "    \"\"\"\n",
    "    \n",
    "    get the estimated parameters (betas/random effect variances) as the 2nd step of ARLMM.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    Y: numpy array, ...\n",
    "\n",
    "    X: numpy array, ...\n",
    "    \n",
    "    grm: numpy array, ...\n",
    "    \n",
    "    measurement: list, ...\n",
    "    \n",
    "    subject_level: list, ...\n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    DX_label: new label of diagnosis, for example 0 = 'early converter' and 1 = 'late converter'\n",
    "    \n",
    "    \"\"\"\n",
    "    N = np.shape(grm)[0]\n",
    "        \n",
    "    longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "    \n",
    "    # Accurate Method \n",
    "    # t_error_cor = np.diag(np.ones(T.shape[0])) + stable_parameter0*longit_C1 + rho_est*stable_parameter0*longit_C2\n",
    "    # t_error_cor = np.diag(np.ones(T.shape[0])) + rho_est*longit_C1 + rho_est*stable_parameter1*longit_C2            \n",
    "    early_convert_index = np.where(DX_label == 0)[0]\n",
    "    late_convert_index = np.where(DX_label == 1)[0]\n",
    "\n",
    "    early_convert_X, early_convert_Y, early_convert_measurement = X[early_convert_index], Y[early_convert_index], measurement[early_convert_index]\n",
    "    early_convert_grm = grm[early_convert_index, :]\n",
    "    early_convert_grm = early_convert_grm[:, early_convert_index]\n",
    "    early_convert_T = np.zeros((np.shape(early_convert_index)[0], np.shape(early_convert_index)[0]))\n",
    "    early_convert_T = mapping_matrix_T(early_convert_measurement)\n",
    "    longit_grm_early_convert, longit_C1_early_convert, longit_C2_early_convert, longit_block_diag_matrix_early_convert, longit_error_early_convert = load_random_effect_info_from_file(grm_early_convert, measurement_early_convert, subject_level_early_convert, T_early_convert)\n",
    "    \n",
    "    late_convert_X, late_convert_Y, late_convert_measurement = X[late_convert_index], Y[late_convert_index], measurement[late_convert_index]\n",
    "    late_convert_grm = grm[late_convert_index, :]\n",
    "    late_convert_grm = late_convert_grm[:, late_convert_index]\n",
    "    late_convert_T = np.zeros((np.shape(late_convert_index)[0], np.shape(late_convert_index)[0]))\n",
    "    late_convert_T = mapping_matrix_T(late_convert_measurement) \n",
    "    longit_grm_late_convert, longit_C1_late_convert, longit_C2_late_convert, longit_block_diag_matrix_late_convert, longit_error_late_convert = load_random_effect_info_from_file(grm_early_convert, measurement_early_convert, subject_level_early_convert, T_early_convert)\n",
    "    \n",
    "    # concate matrices from two groups, we have X and Y concatenated\n",
    "    longit_grm = block_diag(early_convert_grm, late_convert_grm)\n",
    "    T = block_diag(early_convert_T, late_convert_T)\n",
    "    longit_error = block_diag(longit_error_early_convert, longit_error_late_convert)\n",
    "    longit_block_diag_matrix_error = block_diag(longit_block_diag_matrix_early_convert, longit_block_diag_matrix_late_convert)\n",
    "    \n",
    "    late_convert_C = np.ones(late_convert_T.shape[0]) + beta_late_convert*longit_C1_late_convert + beta_late_convert*theta_est_late_convert*longit_C2_late_convert\n",
    "    early_convert_C = np.ones(early_convert_T.shape[0]) + beta_late_convert*longit_C1_late_convert + beta_late_convert*theta_est_late_convert*longit_C2_late_convert\n",
    "    C = block_diag(early_convert_C, late_convert_C)\n",
    "    \n",
    "    # projection matrix P\n",
    "    P = project_covariate_func(X)\n",
    "    \n",
    "    # project out covariates X\n",
    "    project_longit_grm_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_grm),[-1, 1], order = 'F')\n",
    "    project_longit_C_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C), [-1, 1], order = 'F')\n",
    "    project_longit_block_diag_matrix_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_block_diag_matrix), [-1, 1], order = 'F')\n",
    "    project_longit_error_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_error).flatten('F'), [-1, 1], order = 'F')\n",
    "    Y_Y_transpose_1d = np.reshape(np.dot(Y, np.transpose(Y)), [-1, 1], order = 'F')\n",
    "    \n",
    "    project_longit_X_1d = np.concatenate((project_longit_grm_1d, project_longit_C_1d, project_longit_block_diag_matrix_1d, project_longit_error_1d), axis = 1)\n",
    "\n",
    "    clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'huber', fit_intercept= False)\n",
    "    clf.fit(project_longit_X_1d, Y_Y_transpose_1d.ravel())\n",
    "\n",
    "    sigma = clf.coef_\n",
    "    var_t_est = max(sigma[2], 0.00000000001) \n",
    "    var_g_est = max(sigma[0], 0.00000000001) \n",
    "    var_e_est = max(sigma[3], 0.00000000001)\n",
    "    var_c_est = max(sigma[1], 0.00000000001) \n",
    "    h2_est = var_g_est/(var_t_est + var_g_est + var_c_est + var_e_est)\n",
    "    \n",
    "    t_error_cov = var_t_est*longit_block_diag_matrix\n",
    "    error_cov = var_e_est*longit_error\n",
    "    genetic_cov = var_g_est*longit_grm\n",
    "    correlation_cov = var_c_est*longit_block_diag_matrix\n",
    "    total_V = t_error_cov + error_cov + genetic_cov + correlation_cov\n",
    "    \n",
    "    Y_new = np.dot(inv(total_V), Y)\n",
    "    repsonse_new = np.dot(np.transpose(X), Y_new)\n",
    "    X_new = np.dot(inv(total_V), X)\n",
    "    predictor_new = np.dot(np.transpose(X), X_new)\n",
    "    beta_est = np.dot(inv(predictor_new), repsonse_new)\n",
    "    return var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est\n",
    "    # get estimates of all variance components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48d0f1",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model  evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "315559bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    \"\"\"\n",
    "    \n",
    "    predict longitudinal phenotype measures given estimated beta values.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    X: np.array, longittudinal covariate matrix inluding intercept [test data].\n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    y_pred: np.array, predicted \n",
    "    \n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    y_pred = np.matmul(X, beta) # 2d numpy array\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83c3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate the predicted vaues with true values on the test data.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    y_pred: np.array, predicted phenotype values.\n",
    "    y_true: np.array, real phenotype values.\n",
    "    \n",
    "    Return(s)\n",
    "    ----------   \n",
    "    mse: numeric value, mean squared error.\n",
    "    rmse: numeric value, root mean squared error.\n",
    "    mae: numeric value, mean absolute error.\n",
    "    r2: numeric value, coefficient of determination.\n",
    "    \n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52aa209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print t value here\n",
    "def booostrap_stats(booostrap_sample, hypothesis_testing_type = 'two-sided', significance_level = 0.05, df = 100):\n",
    "    \"\"\"\n",
    "    get the p-value for testing SNP of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    CI_upper = np.quantile(booostrap_sample, 1 - significance_level/2)\n",
    "    CI_lower = np.quantile(booostrap_sample, significance_level/2)\n",
    "    t_stats = np.mean(booostrap_sample)/np.std(booostrap_sample)\n",
    "    print(CI_upper)\n",
    "    print(CI_lower)\n",
    "    if hypothesis_testing_type == 'left-sided' or 'right-sided':\n",
    "        p_value = (1-t.cdf(abs(t_stats), df))\n",
    "    if hypothesis_testing_type == 'two-sided':\n",
    "        p_value =2*(1-t.cdf(abs(t_stats), df))\n",
    "    p_value = p_value[0]\n",
    "    return CI_lower, CI_upper, p_value, t_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0be3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_kfold_cross_validation(n_splits, longit_measure, longit_X, grm, measurement, subject_level, T, diagnosis_group = None, stable_parameter = 1.0):\n",
    "    \"\"\"\n",
    "    perform K-fold cross validation.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    n_splits: numeric value, the number of folds for cross validation.\n",
    "    longit_measure: numpy array, one-dimensional left longtidinal measures ordered by the time or spatial points.\n",
    "    \n",
    "    longit_X: numpy array, longtudinal covariates.\n",
    "    grm: numpy array, cross sectional genetic relationship matrix (GRM).\n",
    "    measurement: numpy array, number of measurements for each subject (cross sectional).\n",
    "    subject_level: numpy array, number of measurements for each subject (cross sectional).\n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    Return(s)\n",
    "    ----------   \n",
    "    TRAIN_RMSE: numeric value, RMSE for training data.\n",
    "    TRAIN_MAE: numeric value, MAE for traning data.\n",
    "    TRAIN_R2: numeric value, R2 for traning data.\n",
    "    TEST_RMSE: numeric value, RMSE for test data.\n",
    "    TEST_MAE: numeric value, MAE for test data.\n",
    "    TEST_R2: numeric value, R2 for test data.\n",
    "    \n",
    "    \"\"\"\n",
    "    group_kfold = GroupKFold(n_splits)\n",
    "    TRAIN_MSE = []\n",
    "    TRAIN_RMSE = []\n",
    "    TRAIN_MAE = []\n",
    "    TRAIN_R2 = []\n",
    "    TRAIN_BETA = []\n",
    "    TRAIN_RHO = []\n",
    "    TEST_RHO = []\n",
    "\n",
    "    TEST_MSE = []\n",
    "    TEST_RMSE = []\n",
    "    TEST_MAE = []\n",
    "    TEST_r2 = []\n",
    "    TRAIN_H2 = []\n",
    "    TEST_H2 = []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(longit_X, longit_measure, groups)): \n",
    "        #print(f\" Train: index={train_index}, group={groups[train_index]}\") \n",
    "        #print(f\" Test: index={test_index}, group={groups[test_index]}\") \n",
    "        # train_index and test_index are for longit - groups_train_index and groups_test_index are for cross sectional \n",
    "        groups_train_index = [] \n",
    "        groups_test_index = [] \n",
    "        for i in groups[train_index]: \n",
    "            if i not in groups_train_index: \n",
    "                groups_train_index.append(i)\n",
    "\n",
    "        for j in groups[test_index]:\n",
    "            if j not in groups_test_index:\n",
    "                groups_test_index.append(j)\n",
    "        #print(groups_test_index)\n",
    "        train_longit_measure = longit_measure[train_index]\n",
    "        train_longit_X = longit_X[train_index]\n",
    "        train_grm = grm[groups_train_index, :] # wrong\n",
    "        train_grm = train_grm[:, groups_train_index] # wrong\n",
    "        train_subject_level = [subject_level[i] for i in groups_train_index]\n",
    "        train_measurement = measurement[groups_train_index]\n",
    "        train_T = np.zeros((np.shape(train_index)[0], np.shape(groups_train_index)[0]))\n",
    "        train_T = mapping_matrix_T(train_measurement)\n",
    "\n",
    "        test_longit_measure = longit_measure[test_index]\n",
    "        test_longit_X = longit_X[test_index]\n",
    "        test_grm = grm[groups_test_index, :] # wrong\n",
    "        test_grm = test_grm[:, groups_test_index] # wrong\n",
    "        test_subject_level = [subject_level[i] for i in groups_test_index]\n",
    "        test_measurement = measurement[groups_test_index]\n",
    "        test_T = np.zeros((np.shape(test_index)[0], np.shape(groups_test_index)[0]))\n",
    "        test_T = mapping_matrix_T(test_measurement)\n",
    "        #rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est\n",
    "        if diagnosis_group == 'stable':\n",
    "            train_rho, train_var_g, train_var_t, train_var_c, train_var_e, train_h2, train_beta = ARLMM(train_longit_measure, train_longit_X, train_grm, train_measurement, train_subject_level, train_T)\n",
    "        elif diagnosis_group == 'late_converter':\n",
    "            train_rho, train_var_g, train_var_t, train_var_c, train_var_e, train_h2, train_beta = ARLMM_late_converter(train_longit_measure, train_longit_X, train_grm, train_measurement, train_subject_level, train_T, stable_parameter)\n",
    "\n",
    "        train_mse, train_rmse, train_mae, train_r2 = evaluate(predict(train_longit_X, train_beta), train_longit_measure)\n",
    "        test_mse, test_rmse, test_mae, test_r2 = evaluate(predict(test_longit_X, train_beta), test_longit_measure)\n",
    "        #print(np.shape(predict(train_longit_X, train_beta)))\n",
    "        #predict(train_longit_X, train_beta)\n",
    "        #longit_test_y_pred.append(predict(test_longit_X, train_beta))\n",
    "        TRAIN_MSE.append(train_mse)\n",
    "        TRAIN_RMSE.append(train_rmse)\n",
    "        TRAIN_MAE.append(train_mae)\n",
    "        TRAIN_R2.append(train_r2)\n",
    "        TRAIN_BETA.append(train_beta)\n",
    "        #TRAIN_RHO.append(train_rho)\n",
    "        TRAIN_H2.append(train_h2)\n",
    "        \n",
    "        TEST_MSE.append(test_mse)\n",
    "        TEST_RMSE.append(test_rmse)\n",
    "        TEST_MAE.append(test_mae)\n",
    "        TEST_r2.append(test_r2)\n",
    "    #TRAIN_RHO_MEAN = np.mean(TRAIN_RHO)\n",
    "\n",
    "    # boostrap to stimate p value\n",
    "    #if True: \n",
    "    #if n_splits == n_subject: \n",
    "        #BETA_SNP = np.array(TRAIN_BETA)[:, -1]\n",
    "        #CI_lower_SNP, CI_upper_SNP, p_value_SNP = booostrap_stats(BETA_SNP, hypothesis_testing_type = 'two-sided', significance_level = 0.05, df = np.shape(train_index))# Estimate beta values\n",
    "        #return BETA_SNP, CI_lower_SNP, CI_upper_SNP, p_value_SNP\n",
    "    #else:\n",
    "        #TRAIN_RMSE_MEAN = np.mean(TRAIN_RMSE, axis=0)\n",
    "        #TRAIN_MAE_MEAN = np.mean(TRAIN_MAE, axis=0)\n",
    "        #TRAIN_R2_MEAN = np.mean(TRAIN_R2, axis=0)\n",
    "        #TRAIN_RHO_MEAN = np.mean(TRAIN_RHO, axis=0)\n",
    "    \n",
    "        #TEST_MSE_MEAN = np.mean(TEST_MSE, axis=0)\n",
    "        #TEST_RMSE_MEAN = np.mean(TEST_RMSE, axis=0)\n",
    "        #TEST_MAE_MEAN = np.mean(TEST_MAE, axis=0)\n",
    "        #TEST_R2_MEAN = np.mean(TEST_R2, axis=0)\n",
    "        #TEST_RHO_MEAN = np.mean(TEST_RHO, axis=0)   \n",
    "        #print('MEAN_TRAIN_RMSE:', np.mean(TRAIN_RMSE), 'MEAN_TRAIN_MAE:', np.mean(TRAIN_MAE), 'MEAN_TRAIN_R2:', np.mean(TRAIN_R2))\n",
    "        #print('MEAN_TEST_RMSE:', np.mean(TEST_RMSE), 'MEAN_TEST_MAE:', np.mean(TEST_MAE), 'MEAN_TEST_R2:', np.mean(TEST_R2))\n",
    "    \n",
    "    # Estimate beta values\n",
    "    train_beta_estimate = np.mean(TRAIN_BETA, axis=0)\n",
    "    beta_snp_sample = np.array(TRAIN_BETA)[:, -1]\n",
    "    # calculate z score\n",
    "    CI_lower_SNP, CI_upper_SNP, p_value_SNP, z_score_SNP = booostrap_stats(beta_snp_sample, hypothesis_testing_type = 'two-sided', significance_level = 0.05, df = np.shape(train_index))# Estimate beta values \n",
    "    #print('SNP z score:', z_score_SNP)\n",
    "    # calculate p value\n",
    "    #print('SNP p value:', p_value_SNP)\n",
    "    \n",
    "    # h2 raw values\n",
    "    train_h2_estimate = np.mean(TRAIN_H2) \n",
    "    # h2 p values\n",
    "    CI_lower_h2, CI_upper_h2, p_value_h2, z_score_h2 = booostrap_stats(TRAIN_H2, hypothesis_testing_type = 'two-sided', significance_level = 0.05, df = np.shape(train_index))# Estimate beta values \n",
    "    #print('h2 estimate:', train_h2_estimate)\n",
    "    #print('h2 p value:', p_value_h2)\n",
    "    test_rmse_estimate = np.mean(TEST_RMSE, axis=0)\n",
    "    test_rmse_std = np.std(TEST_RMSE, axis=0)\n",
    "    #print('test_rmse_estimate:', test_rmse_estimate)\n",
    "    #print('test_rmse_std:', test_rmse_std)\n",
    "    \n",
    "    # MAE raw values, mean MAE, and std MAE\n",
    "    test_mae_estimate = np.mean(TEST_MAE, axis=0)\n",
    "    test_mae_std = np.std(TEST_MAE, axis=0)\n",
    "    #print('test_mae_estimate:', test_mae_estimate)\n",
    "    #print('test_mae_std:', test_mae_std)\n",
    "    \n",
    "    # R2 raw values, mean R2, and std R2\n",
    "    test_r2_estimate = np.mean(TEST_r2, axis=0)\n",
    "    test_r2_std = np.std(TEST_r2, axis=0)\n",
    "    #print('test_r2_estimate:', test_r2_estimate)\n",
    "    #print('test_r2_std:', test_r2_std)    \n",
    "    return p_value_SNP, z_score_SNP, TRAIN_H2, train_h2_estimate, p_value_h2, TEST_RMSE, test_rmse_estimate, test_rmse_std, TEST_MAE, test_mae_estimate, test_mae_std, TEST_r2, test_r2_estimate, test_r2_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf70e5",
   "metadata": {},
   "source": [
    "# Save results and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a70dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "#n_splits = n_subject\n",
    "p_value_SNP, z_score_SNP, TRAIN_H2, train_h2_estimate, p_value_h2, TEST_RMSE, test_rmse_estimate, test_mae_std, TEST_MAE, test_mae_estimate, test_mae_std, TEST_r2, test_r2_estimate, test_mae_std = group_kfold_cross_validation(n_splits, longit_measure, longit_X, grm, measurement, subject_level, T, diagnosis_group = 'stable', stable_parameter = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcba4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNP_stats_names = ['p_value_SNP', 'z_score_SNP']\n",
    "#SNP_stats_values = [p_value_SNP, z_score_SNP]\n",
    "\n",
    "#h2_stats_names = ['h2 estimate', 'h2 p value']\n",
    "#h2_stats_values = [train_h2_estimate, p_value_h2]\n",
    "\n",
    "#rmse_stats_names = ['rmse_estimate', 'rmse_std']\n",
    "#rmse_stats_values = [test_rmse_estimate, test_rmse_std]\n",
    "\n",
    "#mae_stats_names = ['mae_estimate', 'mae_std']\n",
    "#mae_stats_values = [test_mae_estimate, test_mae_std]\n",
    "\n",
    "#r2_stats_names = ['r2_estimate', 'r2_std']\n",
    "#r2_stats_values = [test_r2_estimate, test_r2_std]\n",
    "\n",
    "#all_res_names = ['all_h2', 'all RMSE', 'all MAE', 'all r2']\n",
    "#all_res_values = [TRAIN_H2, TEST_RMSE, TEST_MAE, TEST_r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a345c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_SNP = dict(zip(SNP_stats_names, SNP_stats_values))\n",
    "#dict_h2 = dict(zip(h2_stats_names, h2_stats_values))\n",
    "#dict_rmse = dict(zip(rmse_stats_names, rmse_stats_values))\n",
    "#dict_r2 = dict(zip(r2_stats_names, r2_stats_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df8e7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_res = dict_SNP | dict_h2 | dict_rmse | dict_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "978795da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_all_res = dict(zip(all_res_names, all_res_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cec82107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_res)\n",
    "#df_stats = pd.DataFrame(dict_res, index=[0])\n",
    "#print(df_stats)\n",
    "#df_all_stats = pd.DataFrame(dict_all_res)\n",
    "#print(df_all_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7aa60cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(output_path_name + '/' + 'ARLMM_' + ROI_name_list[0][1:] +'_' +'APOE4' +'_' + Diagnosis + file_type_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016f5d5",
   "metadata": {},
   "source": [
    "# run all ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd2d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ROI = ['Laccumb', 'Lamyg', 'Lcaud', 'Lhippo', 'Lpal', 'Lput', 'Lthal']\n",
    "R_ROI = ['Raccumb', 'Ramyg', 'Rcaud', 'Rhippo', 'Rpal', 'Rput', 'Rthal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f613985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_calculation(L_ROI, R_ROI, covar_dynamic_name_list, meausure_time_list, covar_stable_name_list, n_splits):# brain measures (left entorhinal thickavg):\n",
    "    df_pheno_merge, dict_roi_name_L, dict_dynamic_cov_name = load_info_from_file(pheno_merge_name, L_ROI, covar_dynamic_name_list, meausure_time_list)\n",
    "    df_pheno_merge, dict_roi_name_R, dict_dynamic_cov_name = load_info_from_file(pheno_merge_name, R_ROI, covar_dynamic_name_list, meausure_time_list)\n",
    "    #print(dict_roi_name_L)\n",
    "    #print(dict_roi_name_R)\n",
    "    N_ROI = len(L_ROI)\n",
    "    final_result = []\n",
    "    #print(dict_roi_name)\n",
    "    #print(dict_dynamic_cov_name)\n",
    "    #print(N_ROI)   \n",
    "    for i in range(N_ROI):\n",
    "        L_R_ROI_name_list = [L_ROI[i], R_ROI[i]]\n",
    "        #template: measurement, longit_measure = load_pheno_measurement_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "        measurement, longit_measure = load_pheno_measurement_from_file(pheno_merge_name, L_R_ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "        #print(longit_measure)\n",
    "        \n",
    "        # mapping matrix\n",
    "        T = mapping_matrix_T(measurement)\n",
    "        \n",
    "        # create subject level to represent scannner/location specific effects\n",
    "        SubjID = df_pheno_merge[subject_ID_name]\n",
    "        subject_level = [ID[:subject_ID_slice_length] for ID in SubjID]\n",
    "        \n",
    "        # Calcuate the number of participants\n",
    "        # load Genetic Relationship Matrix\n",
    "        n_subject = len(df_pheno_merge)\n",
    "        grm = load_grm_info_from_file(n_subject, filename = grm_name)\n",
    "        grm = np.array(grm)    \n",
    "        #print(grm)\n",
    "        \n",
    "        # longitudinal measures\n",
    "        measurement, longit_measure = load_pheno_measurement_from_file(pheno_merge_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "        \n",
    "        # load ranfom effect information\n",
    "        longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "        #print(longit_C1)\n",
    "        #print(longit_C2)\n",
    "        #print(longit_block_diag_matrix)\n",
    "        \n",
    "        longit_X = load_covar_info_from_file(df_pheno_merge, covar_stable_name_list, dict_dynamic_cov_name, measurement, T)\n",
    "        #longit_X_df = pd.DataFrame(longit_X)\n",
    "        #longit_X_df.to_csv('cov_cMCI_precentral_TH.csv')\n",
    "        \n",
    "        # one time estimate\n",
    "        rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est = ARLMM(longit_measure, longit_X, grm, measurement, subject_level, T)\n",
    "        #print(beta_est)\n",
    "        #print(h2_est)  \n",
    "        \n",
    "        # cross validation\n",
    "        #n_splits = 5\n",
    "        p_value_SNP, z_score_SNP, TRAIN_H2, train_h2_estimate, p_value_h2, TEST_RMSE, test_rmse_estimate, test_rmse_std, TEST_MAE, test_mae_estimate, test_mae_std, TEST_r2, test_r2_estimate, test_r2_std = group_kfold_cross_validation(n_splits, longit_measure, longit_X, grm, measurement, subject_level, T, diagnosis_group = 'stable', stable_parameter = 1.0)\n",
    "\n",
    "        snp_p_value.append(p_value_SNP)\n",
    "        snp_z_score.append(z_score_SNP)\n",
    "        # h2 value\n",
    "        h2.append(train_h2_estimate)\n",
    "        h2_p_value.append(p_value_h2)\n",
    "        # predictive stats \n",
    "        test_rmse.append(test_rmse_estimate)\n",
    "        test_rmse_std_value.append(test_rmse_std)\n",
    "        \n",
    "        test_mae.append(test_mae_estimate)\n",
    "        test_mae_std_value.append(test_rmse_std)\n",
    "        \n",
    "        test_r2.append(test_r2_estimate)  \n",
    "        test_r2_std_value.append(test_r2_std)\n",
    "    return snp_z_score, snp_p_value, h2, h2_p_value, test_rmse, test_rmse_std_value, test_mae, test_mae_std_value, test_r2, test_r2_std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c60c6cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-72.6087709685869\n",
      "-82.38654562433972\n",
      "0.5417821243076115\n",
      "0.24049949793636768\n",
      "-71.66003666539692\n",
      "-80.1008191141736\n",
      "0.39422427191006715\n",
      "0.030155787103812254\n",
      "-67.69489795237345\n",
      "-81.58668080453683\n",
      "0.3664267361119898\n",
      "0.04402306904102971\n",
      "-71.9976786816529\n",
      "-81.79964361148261\n",
      "0.5445731056792308\n",
      "0.25668906404852054\n",
      "-71.75547812228331\n",
      "-78.3116140942996\n",
      "0.3755898727588333\n",
      "1.7496092714482962e-22\n",
      "-72.38961507448873\n",
      "-81.33616381732115\n",
      "0.5070856949636545\n",
      "0.05768696736184206\n",
      "-67.30940985632262\n",
      "-82.27061879408004\n",
      "0.41487392166782133\n",
      "0.025898567933151147\n"
     ]
    }
   ],
   "source": [
    "snp_p_value = []\n",
    "snp_z_score = []\n",
    "\n",
    "h2 = []\n",
    "h2_p_value = []\n",
    "\n",
    "test_rmse = []\n",
    "test_rmse_std_value = []\n",
    "test_mae = []\n",
    "test_mae_std_value = []\n",
    "test_r2 = []\n",
    "test_r2_std_value = []\n",
    "n_splits = 5\n",
    "snp_z_score_all_ROI, snp_p_value_all_ROI, h2_all_ROI, h2_p_value_all_ROI, test_rmse_all_ROI, test_rmse_std_all_ROI, test_mae_all_ROI, test_mae_std_all_ROI, test_r2_all_ROI, test_r2_std_all_ROI = final_calculation(L_ROI, R_ROI, covar_dynamic_name_list, meausure_time_list, covar_stable_name_list, n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a002a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_p_value = []\n",
    "snp_z_score = []\n",
    "\n",
    "h2 = []\n",
    "h2_p_value = []\n",
    "\n",
    "test_rmse = []\n",
    "test_rmse_std_value = []\n",
    "test_mae = []\n",
    "test_mae_std_value = []\n",
    "test_r2 = []\n",
    "test_r2_std_value = []\n",
    "n_splits = 5\n",
    "snp_z_score_cv, snp_p_value_cv, h2_cv, h2_p_value_cv, test_rmse_cv, test_rmse_std_cv, test_mae_cv, test_mae_std_cv, test_r2_cv, test_r2_std_cv = final_calculation(L_ROI, R_ROI, covar_dynamic_name_list, meausure_time_list, covar_stable_name_list, n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a56ef106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snp_z_score: [-18.945657531659162, -23.567059875999174, -14.000242517839677, -20.760942949288324, -28.697624631190596, -22.165779549788184, -13.81195360753005]\n",
      "snp_p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "h2: [0.3958208345957287, 0.27927344077807803, 0.25392159405509507, 0.3600519533687058, 0.13173837405940123, 0.33200280993738407, 0.25315903592275896]\n",
      "h2_p_value: [0.000677130090491529, 0.051685832153836486, 0.03788672985290287, 0.0013067667744841582, 0.42596340463337223, 0.05776803131057906, 0.06998770779104535]\n",
      "test_rmse: [252.20983133908226, 252.2345965717171, 252.23524686234182, 252.24802128462207, 252.23072061322355, 252.23892442013366, 252.24855381650787]\n",
      "test_rmse_std: [9.168946540457362, 9.156383566811035, 9.136756555181503, 9.192914542967708, 9.136164628671587, 9.153500961076126, 9.177982176354305]\n",
      "test_mae: [201.0097275453027, 201.02622538154657, 201.02170378405253, 201.03225988057116, 201.01590700932877, 201.04508043255345, 201.03763119207073]\n",
      "test_mae_std: [9.168946540457362, 9.156383566811035, 9.136756555181503, 9.192914542967708, 9.136164628671587, 9.153500961076126, 9.177982176354305]\n",
      "test_r2: [0.10059966181812899, 0.10042665458573483, 0.10041871777464258, 0.10033943428024901, 0.10045161347174322, 0.10039605918180419, 0.10033226827935977]\n",
      "test_r2_std: [0.04107794084608479, 0.04092418674917248, 0.04089092067078332, 0.04093719558885678, 0.040871401673617136, 0.04090366712619362, 0.04092978848690945]\n"
     ]
    }
   ],
   "source": [
    "print('snp_z_score:', snp_z_score_all_ROI)\n",
    "print('snp_p_value:', snp_p_value_all_ROI)\n",
    "print('h2:', h2_all_ROI)\n",
    "print('h2_p_value:', h2_p_value_all_ROI)\n",
    "print('test_rmse:', test_rmse_cv)\n",
    "print('test_rmse_std:', test_rmse_std_cv)\n",
    "print('test_mae:', test_mae_cv)\n",
    "print('test_mae_std:', test_mae_std_cv)\n",
    "print('test_r2:', test_r2_cv)\n",
    "print('test_r2_std:', test_r2_std_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b74bb36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accumbens': -18.945657531659162, 'amygdala': -23.567059875999174, 'caudate': -14.000242517839677, 'hippocampus': -20.760942949288324, 'pallidum': -28.697624631190596, 'putamen': -22.165779549788184, 'thalamus': -13.81195360753005}\n",
      "{'accumbens': 0.0, 'amygdala': 0.0, 'caudate': 0.0, 'hippocampus': 0.0, 'pallidum': 0.0, 'putamen': 0.0, 'thalamus': 0.0}\n",
      "{'accumbens': 0.3958208345957287, 'amygdala': 0.27927344077807803, 'caudate': 0.25392159405509507, 'hippocampus': 0.3600519533687058, 'pallidum': 0.13173837405940123, 'putamen': 0.33200280993738407, 'thalamus': 0.25315903592275896}\n",
      "{'accumbens': 0.000677130090491529, 'amygdala': 0.051685832153836486, 'caudate': 0.03788672985290287, 'hippocampus': 0.0013067667744841582, 'pallidum': 0.42596340463337223, 'putamen': 0.05776803131057906, 'thalamus': 0.06998770779104535}\n",
      "{'accumbens': 252.20983133908226, 'amygdala': 252.2345965717171, 'caudate': 252.23524686234182, 'hippocampus': 252.24802128462207, 'pallidum': 252.23072061322355, 'putamen': 252.23892442013366, 'thalamus': 252.24855381650787}\n",
      "{'accumbens': 9.168946540457362, 'amygdala': 9.156383566811035, 'caudate': 9.136756555181503, 'hippocampus': 9.192914542967708, 'pallidum': 9.136164628671587, 'putamen': 9.153500961076126, 'thalamus': 9.177982176354305}\n",
      "{'accumbens': 201.0097275453027, 'amygdala': 201.02622538154657, 'caudate': 201.02170378405253, 'hippocampus': 201.03225988057116, 'pallidum': 201.01590700932877, 'putamen': 201.04508043255345, 'thalamus': 201.03763119207073}\n",
      "{'accumbens': 9.168946540457362, 'amygdala': 9.156383566811035, 'caudate': 9.136756555181503, 'hippocampus': 9.192914542967708, 'pallidum': 9.136164628671587, 'putamen': 9.153500961076126, 'thalamus': 9.177982176354305}\n",
      "{'accumbens': 0.10059966181812899, 'amygdala': 0.10042665458573483, 'caudate': 0.10041871777464258, 'hippocampus': 0.10033943428024901, 'pallidum': 0.10045161347174322, 'putamen': 0.10039605918180419, 'thalamus': 0.10033226827935977}\n",
      "{'accumbens': 0.04107794084608479, 'amygdala': 0.04092418674917248, 'caudate': 0.04089092067078332, 'hippocampus': 0.04093719558885678, 'pallidum': 0.040871401673617136, 'putamen': 0.04090366712619362, 'thalamus': 0.04092978848690945}\n"
     ]
    }
   ],
   "source": [
    "ROI_name = ['accumbens', 'amygdala', 'caudate', 'hippocampus', 'pallidum', 'putamen', 'thalamus']\n",
    "\n",
    "dict_snp_z_score_value = dict(zip(ROI_name, snp_z_score_all_ROI))\n",
    "dict_snp_p_value = dict(zip(ROI_name, snp_p_value_all_ROI))\n",
    "\n",
    "dict_h2_value = dict(zip(ROI_name, h2_all_ROI))\n",
    "dict_h2_p_value = dict(zip(ROI_name, h2_p_value_all_ROI))\n",
    "\n",
    "dict_rmse_value = dict(zip(ROI_name, test_rmse_cv))\n",
    "dict_rmse_std_value = dict(zip(ROI_name, test_rmse_std_cv))\n",
    "\n",
    "dict_mae_value = dict(zip(ROI_name, test_mae_cv))\n",
    "dict_mae_std_value = dict(zip(ROI_name, test_mae_std_cv))\n",
    "\n",
    "dict_r2_value = dict(zip(ROI_name, test_r2_cv))\n",
    "dict_r2_std_value = dict(zip(ROI_name, test_r2_std_cv))\n",
    "print(dict_snp_z_score_value)\n",
    "print(dict_snp_p_value)\n",
    "print(dict_h2_value)\n",
    "print(dict_h2_p_value)\n",
    "\n",
    "print(dict_rmse_value)\n",
    "print(dict_rmse_std_value)\n",
    "\n",
    "print(dict_mae_value)\n",
    "print(dict_mae_std_value)\n",
    "\n",
    "print(dict_r2_value)\n",
    "print(dict_r2_std_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d92dbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accumbens  amygdala   caudate  hippocampus  pallidum   putamen  thalamus\n",
      "All     0.1006  0.100427  0.100419     0.100339  0.100452  0.100396  0.100332\n",
      "     accumbens  amygdala   caudate  hippocampus  pallidum   putamen  thalamus\n",
      "All   0.041078  0.040924  0.040891     0.040937  0.040871  0.040904   0.04093\n"
     ]
    }
   ],
   "source": [
    "df_snp_z_score_value = pd.DataFrame(dict_snp_z_score_value, index = [Diagnosis])\n",
    "df_snp_p_value = pd.DataFrame(dict_snp_p_value, index = [Diagnosis])\n",
    "\n",
    "df_h2_value = pd.DataFrame(dict_h2_value, index = [Diagnosis])\n",
    "df_h2_p_value = pd.DataFrame(dict_h2_p_value, index = [Diagnosis])\n",
    "\n",
    "df_rmse_value = pd.DataFrame(dict_rmse_value, index = [Diagnosis])\n",
    "df_rmse_std_value = pd.DataFrame(dict_rmse_std_value, index = [Diagnosis])\n",
    "\n",
    "df_mae_value = pd.DataFrame(dict_mae_value, index = [Diagnosis])\n",
    "df_mae_std_value = pd.DataFrame(dict_mae_std_value, index = [Diagnosis])\n",
    "\n",
    "df_r2_value = pd.DataFrame(dict_r2_value, index = [Diagnosis])\n",
    "df_r2_std_value = pd.DataFrame(dict_r2_std_value, index = [Diagnosis])\n",
    "\n",
    "print(df_r2_value)\n",
    "print(df_r2_std_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "108b6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snp_z_score_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'snp_z_score' + '_' + Diagnosis + file_type_name)\n",
    "df_snp_p_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'snp_p_value' + '_' + Diagnosis + file_type_name)\n",
    "\n",
    "df_h2_value.to_csv(output_path_name + '/' + 'ARLMM' + '_' + 'h2_value' + '_' + Diagnosis + file_type_name)\n",
    "df_h2_p_value.to_csv(output_path_name + '/' + 'ARLMM' + '_' + 'h2_p_value' + '_' + Diagnosis + file_type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7510d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rmse_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'RMSE' + '_' + Diagnosis + file_type_name)\n",
    "df_rmse_std_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'RMSE_std' + '_' + Diagnosis + file_type_name)\n",
    "\n",
    "df_mae_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'MAE' + '_' + Diagnosis + file_type_name)\n",
    "df_mae_std_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'MAE_std' + '_' + Diagnosis + file_type_name)\n",
    "\n",
    "df_r2_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'R2' + '_' + Diagnosis + file_type_name)\n",
    "df_r2_std_value.to_csv(output_path_name + '/' + 'ARLMM' +'_' + 'R2_std' + '_' + Diagnosis + file_type_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef1c02",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cb6c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bc75466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8da314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation metrics\n",
    "mse_train_scores = []\n",
    "mae_train_scores = []\n",
    "r2_train_scores = []\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "mse_test_scores = []\n",
    "mae_test_scores = []\n",
    "r2_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e31c1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(longit_X):\n",
    "    X_train, X_test = longit_X[train_index], longit_X[test_index]\n",
    "    y_train, y_test = longit_measure[train_index], longit_measure[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    model = sm.OLS(y_train, X_train)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = results.predict(X_test)\n",
    "    y_train_pred = results.predict(X_train)\n",
    "    # Calculate evaluation metrics\n",
    "    mse_test_scores.append(mean_squared_error(y_test, y_pred))\n",
    "    mae_test_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "    r2_test_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    mse_train_scores.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mae_train_scores.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    r2_train_scores.append(r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b74fc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249.30824361591\n",
      "198.77728463738487\n",
      "0.1254549613298589\n",
      "251.05181820448752\n",
      "199.80529359635946\n",
      "0.11150913367996437\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(np.mean(mse_train_scores)))\n",
    "print(np.mean(mae_train_scores))\n",
    "print(np.mean(r2_train_scores))\n",
    "\n",
    "print(np.sqrt(np.mean(mse_test_scores)))\n",
    "print(np.mean(mae_test_scores))\n",
    "print(np.mean(r2_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70941bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
