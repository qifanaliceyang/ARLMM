{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29f515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from scipy.linalg import block_diag, inv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3703becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input info\n",
    "ROI_name_list = ['L_precentral_thickavg', 'R_precentral_thickavg']\n",
    "covar_dynamic_name_list = ['Age', 'Sex'] \n",
    "# recode sex info!\n",
    "covar_stable_name_list = ['APOE4'] \n",
    "meausure_time_list = ['sc', '12mo', '24mo']\n",
    "#n_measure_point = len(meausure_time_list)\n",
    "#n_timepoint = 3\n",
    "#slice_length = 3\n",
    "\n",
    "#genetic information\n",
    "grm_path = None\n",
    "\n",
    "# Input files\n",
    "# pycaret package ~\n",
    "path_name = '/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Cortical_Thickness_SurfArea_Final'\n",
    "pheno_file_name = '/ADNI1plus2_cortical_CN_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_MCI_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_Dementia_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_MCI2Dementia_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_MCIDementia2_final_sort.csv'\n",
    "#pheno_file_name = '/ADNI1plus2_cortical_Converter_final_sort.csv'\n",
    "pheno_merge_name = path_name + pheno_file_name\n",
    "#diagnosis_name = 'sMCI' # diagnosis name may be extracted from file name\n",
    "\n",
    "# output path name\n",
    "output_path_name = '/Users/aliceyang/result'\n",
    "output_file_name = 'res'\n",
    "file_type_name = '.csv'\n",
    "\n",
    "# the time point/spatial location ifo should not be coded in the subject ID!\n",
    "# cohort info\n",
    "subject_ID_name = 'SubjID'\n",
    "subject_ID_slice_length = 3\n",
    "stable_parameter = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac000b",
   "metadata": {},
   "source": [
    "# Read files and load column information to the corresponding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ffa91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_info_from_file(filename, ROI_name_list, covar_dynamic_name_list, meausure_time_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    read input csv file containing all phenotype/genotype/covariates such as age and sex.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------\n",
    "    filename: str, csv filename.\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    df: dataframe of the csv file.\n",
    "    \n",
    "    n: total number of measurements for all subjects.\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(filename)\n",
    "    #print(ROI_name_list)\n",
    "    #print(covar_dynamic_name_list)\n",
    "    #print(meausure_time_list)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        print('file not found!')\n",
    "        sys.exit()\n",
    "    # n = df.shape[0]\n",
    "    \n",
    "    dict_roi_name = {}\n",
    "    for roi in ROI_name_list:\n",
    "        dict_roi_name[roi] = []\n",
    "        for tp in meausure_time_list:\n",
    "            col_name = roi + '_' + tp\n",
    "            if col_name not in df.columns:\n",
    "                print(col_name + 'does not exist!')\n",
    "                sys.exit()\n",
    "            dict_roi_name[roi].append(col_name)\n",
    "    #print(dict_roi_name)\n",
    "    \n",
    "    dict_dynamic_cov_name = {}\n",
    "    for cov in covar_dynamic_name_list:\n",
    "        dict_dynamic_cov_name[cov] = []\n",
    "        for tp in meausure_time_list:\n",
    "            col_name = cov + '_' + tp\n",
    "            if col_name not in df.columns:\n",
    "                print(col_name + 'does not exist!')\n",
    "                sys.exit()\n",
    "            dict_dynamic_cov_name[cov].append(col_name)\n",
    "    #print(dict_dynamic_cov_name)\n",
    "    return df, dict_roi_name, dict_dynamic_cov_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c1bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_matrix_T(measurement):\n",
    "    \"\"\"\n",
    "    \n",
    "    get mapping matrix to map cross sectional varibles (covariates/ranfom effect correlation) to longitudinal information.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------    \n",
    "    measurement: list, each entry represents the number of measurements for each subject.\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}. \n",
    "    \n",
    "    \"\"\"\n",
    "    T = np.expand_dims(np.repeat(1, measurement[0]), axis=1)\n",
    "    for i in range(1, np.shape(measurement)[0]): \n",
    "        T_pre = T\n",
    "        T = block_diag(T_pre, np.expand_dims(np.repeat(1, measurement[i]), axis = 1))\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2ef913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_longit_mapping_function(matrix_T, correlation_matrix):\n",
    "    \"\"\"\n",
    "    \n",
    "    map function for cross-sectional covariate information or random effect correlation information to longitudinal.\n",
    "    T=blkdiag{1_n1 , ···,1_nm} \n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------  \n",
    "    correlation_matrix: numpy array N x N, the correlation matrix specified for the normal distribution. N number of subjects\n",
    "    \n",
    "    matrix_T: numpy array ? x ?, the function to map cross sectional to longitudinal.\n",
    "    \n",
    "\n",
    "    \n",
    "    Return(s): \n",
    "    ----------\n",
    "    longit_correlation: longitudinal correlation matrix mapped from cross sectional \n",
    "    \n",
    "    \"\"\"\n",
    "    # use @ \n",
    "    T_correlation = np.matmul(matrix_T, correlation_matrix)\n",
    "    T_correlation_T_transpose = np.matmul(T_correlation, np.transpose(matrix_T))\n",
    "    N_total_measurement = np.shape(T_correlation_T_transpose)[0]\n",
    "    longit_correlation = T_correlation_T_transpose\n",
    "    #longit_correlation_1d = T_correlation_T_transpose.flatten('F')\n",
    "    return longit_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c50f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grm_info_from_file(n_subject, filename):\n",
    "    \"\"\"\n",
    "       ! optional \n",
    "    \n",
    "    \"\"\"\n",
    "    if filename == None:\n",
    "        grm = np.identity(n_subject)\n",
    "    else:\n",
    "        grm = pd.read_csv(filename, header = None)\n",
    "    return grm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f56d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_basic_subject_measurement_info_from_file(n_subject, filename, ROI_name_list, ROI_index = None):\n",
    "    # how to caluculate the number of measurements for each subject?\n",
    "    if filename == None:\n",
    "        measurement = df.columns[df.columns.str.startswith(ROI_name_list[ROI_index])]\n",
    "    else:\n",
    "        measurement = pd.read_csv(filename, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7191a0",
   "metadata": {},
   "source": [
    "# Generate random effect correlation matrix information and extract covariance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a1d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_effect_info_from_file(grm, measurement, subject_level, T):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    get the random effect covariance matrix for each of the ranfom effect (genetic relationship/temporal or spatial effect/site effect/measurement error)\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    grm: numpy array, input Genetic Relationship Matrix (GRM) for subject-subject correlation.\n",
    "    \n",
    "    measurement: list, each entry represents the number of measurements for any subject.\n",
    "    \n",
    "    subject_level: numpy array, \n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    Return(s)\n",
    "    ----------  \n",
    "    longit_grm: numpy array, one-dimensional array for longitudinal Genetic Relationship Matrix (GRM).\n",
    "    \n",
    "    longit_C1: numpy array,  one-dimensional array for longitudinal correlation matrix corresponding to time intervals between two closest measurements. \n",
    "    \n",
    "    longit_C2: numpy array,  one-dimensional array for longitudinal correlation matrix corresponding to time intervals between two closest measurements.\n",
    "    \n",
    "    longit_block_diag_matrix: numpy array,  one-dimensional array for longitudinal block diagnoal matrix \n",
    "    \n",
    "    longit_error: numpy array, one-dimensional array for indepedent measurement erors.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "# Create longitudinal GRM 1d info\n",
    "    grm_arr = np.array(grm)\n",
    "    longit_grm = cross_sectional_longit_mapping_function(T, grm_arr)\n",
    "    \n",
    "# Create covariate matrix input for temporal or spatialy correlated random effect using time intervals or distance functions\n",
    "# hypothetically  \n",
    "    if measurement is not None:# Create T always 3 time points\n",
    "        longit_C1 = np.zeros((measurement[0], measurement[0]))\n",
    "        longit_C2 = np.zeros((measurement[0], measurement[0]))\n",
    "        for i in range(measurement[0]):\n",
    "            for j in range(measurement[0]):\n",
    "                    if abs(i - j) == 1:\n",
    "                        longit_C1[i, j] = 1 # rho**abs(i - j) \n",
    "                    if abs(i - j) == 2:\n",
    "                        longit_C2[i, j] = 1 # rho**abs(i - j)\n",
    "    \n",
    "        for s in range(1, len(measurement)):\n",
    "            longit_C1_temp = np.zeros((measurement[s], measurement[s]))\n",
    "            longit_C2_temp = np.zeros((measurement[s], measurement[s]))\n",
    "            for i in range(measurement[s]):\n",
    "                for j in range(measurement[s]):\n",
    "                    if abs(i - j) == 1:\n",
    "                        longit_C1_temp[i, j] = 1 # rho**abs(i - j) \n",
    "                    if abs(i - j) == 2:\n",
    "                        longit_C2_temp[i, j] = 1 # rho**abs(i - j)\n",
    "            # C1 definition\n",
    "            longit_C1_pre = longit_C1\n",
    "            longit_C1 = block_diag(longit_C1_pre, longit_C1_temp)\n",
    "            # C2 definition\n",
    "            longit_C2_pre = longit_C2\n",
    "            longit_C2 = block_diag(longit_C2_pre, longit_C2_temp)\n",
    "        \n",
    "# Create longitudinal site 1d info using subject level\n",
    "    subject_level_dummy = pd.get_dummies(subject_level)\n",
    "    subject_level_cnt = subject_level_dummy.sum(axis=0)\n",
    "    block_diag_matrix = np.identity(subject_level_cnt[0])\n",
    "    for i in range(1, np.shape(subject_level_cnt)[0]):\n",
    "            block_diag_matrix_prev = block_diag_matrix\n",
    "            block_diag_matrix = block_diag(block_diag_matrix_prev, np.identity(subject_level_cnt[i]))\n",
    "    longit_block_diag_matrix = cross_sectional_longit_mapping_function(T, block_diag_matrix)\n",
    "    \n",
    "# create measurement error 1d info\n",
    "    longit_error = np.identity(np.sum(measurement))\n",
    "    return longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a115345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covar_info_from_file(df, covar_stable_name_list, dict_dynamic_cov_name, measurement, T):\n",
    "    \n",
    "    intercept = np.expand_dims(np.ones(np.sum(measurement)), axis=1)\n",
    "    if covar_stable_name_list is not None:\n",
    "        covar_stable_matrix = np.zeros((len(measurement),len(covar_stable_name_list)))\n",
    "        i = 0\n",
    "        for cov in covar_stable_name_list:\n",
    "            cov_value = df[cov]\n",
    "            print(cov)\n",
    "            covar_stable_matrix[:, i] = cov_value\n",
    "            i = i + 1\n",
    "        covar_stable_matrix_longit = np.matmul(T, covar_stable_matrix)\n",
    "        if dict_dynamic_cov_name is None:\n",
    "            covar_stable_matrix_longit = np.concatenate((intercept,covar_stable_matrix_longit), axis = 1) \n",
    "            return covar_stable_matrix_longit\n",
    "    \n",
    "    if dict_dynamic_cov_name is not None:\n",
    "        covar_dynamic_matrix_longit = np.zeros((sum(measurement), len(dict_dynamic_cov_name)))\n",
    "        j = 0\n",
    "        for cov in dict_dynamic_cov_name.keys():\n",
    "            cov_longit_value = []\n",
    "            for cov_longit in dict_dynamic_cov_name[cov]:\n",
    "                cov_longit_value.append(df[cov_longit])\n",
    "            cov_longit_value = np.array(cov_longit_value).flatten('F')\n",
    "            covar_dynamic_matrix_longit[:, j] = cov_longit_value\n",
    "            j = j + 1\n",
    "        if covar_stable_name_list is None:\n",
    "            covar_dynamic_matrix_longit = np.concatenate((intercept,covar_dynamic_matrix_longit), axis = 1)\n",
    "            return covar_dynamic_matrix_longit\n",
    "        \n",
    "        covar_matrix_longit = np.concatenate((intercept, covar_dynamic_matrix_longit, covar_stable_matrix_longit), axis = 1)\n",
    "        return covar_matrix_longit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5adf7",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model parameter estimation: 2-step method\n",
    "step 1: project out covariance matrix X including age, sex, SNP encoding.\n",
    "\n",
    "step 2: run support vector regression to estimate random effect components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58143472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step - Create projection matrix to regress out covariance matrix X\n",
    "\n",
    "def project_covariate_func(X):\n",
    "    \"\"\"\n",
    "    \n",
    "    get the covariates matrix when the users provide the name of coavariates needed.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    X: np.array, longittudinal covariate matrix inluding intercept.\n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    P: Projection matrix, so that X is projected to 0 matrix through P.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    XX = np.dot(np.transpose(X), X)\n",
    "    Z = np.dot(X, inv(XX))\n",
    "    P = np.diag(np.ones(X.shape[0])) - np.dot(Z, np.transpose(X))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2073dc",
   "metadata": {},
   "source": [
    "# Autoregressive Mixed Model (converter group) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e96ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARLMM_late_converter(Y, X, grm, measurement, subject_level, T, stable_parameter):\n",
    "    # notice for late converter group the correlation matrix used are the same as stable group \n",
    "    # but stable parameter is known\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    get the estimated parameters (betas/random effect variances) as the 2nd step of ARLMM.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    Y: numpy array, one-dimensional longtidinal measures ordered by the time or spatial points.\n",
    "\n",
    "    X: numpy array, longitudinal covariate matrix inluding intercept.\n",
    "    \n",
    "    grm: numpy array, cross sectional genetic relationship matrix.\n",
    "    \n",
    "    measurement: list, each entry represents the number of measurements for any subject.\n",
    "    \n",
    "    subject_level: list, ...\n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    stable_parameter: numeric, \n",
    "    \n",
    "    \n",
    "\n",
    "    Return(s)\n",
    "    ----------   \n",
    "    rho_est: numeric value, correlation parameter for temporal/spatial correlation matrix (converter).\n",
    "    \n",
    "    var_g_est: numeric value, variance of genetic effect.\n",
    "    \n",
    "    var_t_est: numeric value, variance of imaging site effect.\n",
    "    \n",
    "    var_c_est: numeric value, variance of temporal/spatial correlated effect.\n",
    "    \n",
    "    var_e_est: numeric value, variance of meausurement error.\n",
    "    \n",
    "    h2_est: numeric value, proportion of genetic effect over total effect.\n",
    "    \n",
    "    beta_est: numpy array, beta values for covariance.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    P = project_covariate_func(X)\n",
    "    \n",
    "    longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "    \n",
    "    project_longit_grm_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_grm),[-1, 1], order = 'F')\n",
    "    project_longit_C1_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C1), [-1, 1], order = 'F')\n",
    "    project_longit_C2_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C2), [-1, 1], order = 'F')\n",
    "    project_longit_block_diag_matrix_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_block_diag_matrix), [-1, 1], order = 'F')\n",
    "    project_longit_error_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_error).flatten('F'), [-1, 1], order = 'F')\n",
    "    Y_Y_transpose_1d = np.reshape(np.dot(Y, np.transpose(Y)), [-1, 1], order = 'F')\n",
    "    \n",
    "    project_longit_X_1d = np.concatenate((project_longit_grm_1d, project_longit_C1_1d, project_longit_C2_1d, project_longit_block_diag_matrix_1d, project_longit_error_1d), axis = 1)\n",
    "\n",
    "    clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'squared_epsilon_insensitive', fit_intercept= False)\n",
    "    clf.fit(project_longit_X_1d, Y_Y_transpose_1d.ravel())\n",
    "    \n",
    "    sigma = clf.coef_\n",
    "    rho_est = max(0.00000000001, min(sigma[2]/sigma[1], 1)) \n",
    "    var_t_est = max(0.5*sigma[1]/stable_parameter + 0.5*sigma[2]/(rho_est*stable_parameter), 0.00000000001) \n",
    "    var_g_est = max(sigma[0], 0.00000000001) \n",
    "    var_e_est = max(sigma[3] - var_t_est, 0.00000000001)\n",
    "    var_c_est = max(sigma[4], 0.00000000001) \n",
    "    h2_est = var_g_est/(var_t_est + var_g_est + var_c_est + var_e_est)\n",
    "    \n",
    "    t_error_cov = var_t_est*(np.diag(np.ones(T.shape[0])) + stable_parameter*longit_C1 + rho_est*stable_parameter*longit_C2)\n",
    "    error_cov = var_e_est*np.diag(np.ones(T.shape[0]))\n",
    "    genetic_cov = var_g_est*longit_grm\n",
    "    correlation_cov = var_c_est*longit_block_diag_matrix\n",
    "    total_V = t_error_cov + error_cov + genetic_cov + correlation_cov\n",
    "    \n",
    "    Y_new = np.dot(inv(total_V), Y)\n",
    "    repsonse_new = np.dot(np.transpose(X), Y_new)\n",
    "    X_new = np.dot(inv(total_V), X)\n",
    "    predictor_new = np.dot(np.transpose(X), X_new)\n",
    "    beta_est = np.dot(inv(predictor_new), repsonse_new)\n",
    "    \n",
    "    return rho_est, var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f260797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARLMM_total_converter(Y, X, grm, measurement, subject_level, T, beta_late_convert):\n",
    "    \"\"\"\n",
    "    \n",
    "    get the estimated parameters (betas/random effect variances) as the 2nd step of ARLMM.\n",
    "    \n",
    "    Parameter(s)\n",
    "    ----------   \n",
    "    Y: numpy array, ...\n",
    "\n",
    "    X: numpy array, ...\n",
    "    \n",
    "    grm: numpy array, ...\n",
    "    \n",
    "    measurement: list, ...\n",
    "    \n",
    "    subject_level: list, ...\n",
    "    \n",
    "    T: numpy array, the mapping matrix T, T=blkdiag{1_n1 , ···,1_nm}.\n",
    "    \n",
    "    DX_label: new label of diagnosis, for example 0 = 'early converter' and 1 = 'late converter'\n",
    "    \n",
    "    \"\"\"\n",
    "    N = np.shape(grm)[0]\n",
    "        \n",
    "    longit_grm, longit_C1, longit_C2, longit_block_diag_matrix, longit_error = load_random_effect_info_from_file(grm, measurement, subject_level, T)\n",
    "    \n",
    "    # Accurate Method \n",
    "    # t_error_cor = np.diag(np.ones(T.shape[0])) + stable_parameter0*longit_C1 + rho_est*stable_parameter0*longit_C2\n",
    "    # t_error_cor = np.diag(np.ones(T.shape[0])) + rho_est*longit_C1 + rho_est*stable_parameter1*longit_C2            \n",
    "    early_convert_index = np.where(DX_label == 0)[0]\n",
    "    late_convert_index = np.where(DX_label == 1)[0]\n",
    "\n",
    "    early_convert_X, early_convert_Y, early_convert_measurement = X[early_convert_index], Y[early_convert_index], measurement[early_convert_index]\n",
    "    early_convert_grm = grm[early_convert_index, :]\n",
    "    early_convert_grm = early_convert_grm[:, early_convert_index]\n",
    "    early_convert_T = np.zeros((np.shape(early_convert_index)[0], np.shape(early_convert_index)[0]))\n",
    "    early_convert_T = mapping_matrix_T(early_convert_measurement)\n",
    "    longit_grm_early_convert, longit_C1_early_convert, longit_C2_early_convert, longit_block_diag_matrix_early_convert, longit_error_early_convert = load_random_effect_info_from_file(grm_early_convert, measurement_early_convert, subject_level_early_convert, T_early_convert)\n",
    "    \n",
    "    late_convert_X, late_convert_Y, late_convert_measurement = X[late_convert_index], Y[late_convert_index], measurement[late_convert_index]\n",
    "    late_convert_grm = grm[late_convert_index, :]\n",
    "    late_convert_grm = late_convert_grm[:, late_convert_index]\n",
    "    late_convert_T = np.zeros((np.shape(late_convert_index)[0], np.shape(late_convert_index)[0]))\n",
    "    late_convert_T = mapping_matrix_T(late_convert_measurement) \n",
    "    longit_grm_late_convert, longit_C1_late_convert, longit_C2_late_convert, longit_block_diag_matrix_late_convert, longit_error_late_convert = load_random_effect_info_from_file(grm_early_convert, measurement_early_convert, subject_level_early_convert, T_early_convert)\n",
    "    \n",
    "    # concate matrices from two groups, we have X and Y concatenated\n",
    "    longit_grm = block_diag(early_convert_grm, late_convert_grm)\n",
    "    T = block_diag(early_convert_T, late_convert_T)\n",
    "    longit_error = block_diag(longit_error_early_convert, longit_error_late_convert)\n",
    "    longit_block_diag_matrix_error = block_diag(longit_block_diag_matrix_early_convert, longit_block_diag_matrix_late_convert)\n",
    "    \n",
    "    late_convert_C = np.ones(late_convert_T.shape[0]) + beta_late_convert*longit_C1_late_convert + beta_late_convert*theta_est_late_convert*longit_C2_late_convert\n",
    "    early_convert_C = np.ones(early_convert_T.shape[0]) + beta_late_convert*longit_C1_late_convert + beta_late_convert*theta_est_late_convert*longit_C2_late_convert\n",
    "    C = block_diag(early_convert_C, late_convert_C)\n",
    "    \n",
    "    # projection matrix P\n",
    "    P = project_covariate_func(X)\n",
    "    \n",
    "    # project out covariates X\n",
    "    project_longit_grm_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_grm),[-1, 1], order = 'F')\n",
    "    project_longit_C_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_C), [-1, 1], order = 'F')\n",
    "    project_longit_block_diag_matrix_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_block_diag_matrix), [-1, 1], order = 'F')\n",
    "    project_longit_error_1d = np.reshape(cross_sectional_longit_mapping_function(P, longit_error).flatten('F'), [-1, 1], order = 'F')\n",
    "    Y_Y_transpose_1d = np.reshape(np.dot(Y, np.transpose(Y)), [-1, 1], order = 'F')\n",
    "    \n",
    "    project_longit_X_1d = np.concatenate((project_longit_grm_1d, project_longit_C_1d, project_longit_block_diag_matrix_1d, project_longit_error_1d), axis = 1)\n",
    "\n",
    "    clf = SGDRegressor(tol = 1e-3, penalty = 'l2',loss = 'squared_epsilon_insensitive', fit_intercept= False)\n",
    "    clf.fit(project_longit_X_1d, Y_Y_transpose_1d.ravel())\n",
    "\n",
    "    sigma = clf.coef_\n",
    "    var_t_est = max(sigma[2], 0.00000000001) \n",
    "    var_g_est = max(sigma[0], 0.00000000001) \n",
    "    var_e_est = max(sigma[3], 0.00000000001)\n",
    "    var_c_est = max(sigma[1], 0.00000000001) \n",
    "    h2_est = var_g_est/(var_t_est + var_g_est + var_c_est + var_e_est)\n",
    "    \n",
    "    t_error_cov = var_t_est*longit_block_diag_matrix\n",
    "    error_cov = var_e_est*longit_error\n",
    "    genetic_cov = var_g_est*longit_grm\n",
    "    correlation_cov = var_c_est*longit_block_diag_matrix\n",
    "    total_V = t_error_cov + error_cov + genetic_cov + correlation_cov\n",
    "    \n",
    "    Y_new = np.dot(inv(total_V), Y)\n",
    "    repsonse_new = np.dot(np.transpose(X), Y_new)\n",
    "    X_new = np.dot(inv(total_V), X)\n",
    "    predictor_new = np.dot(np.transpose(X), X_new)\n",
    "    beta_est = np.dot(inv(predictor_new), repsonse_new)\n",
    "    return var_g_est, var_t_est, var_c_est, var_e_est, h2_est, beta_est\n",
    "    # get estimates of all variance components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3e08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_file_name_late_convert = '/ADNI1plus2_cortical_MCI2Dementia_final_sort.csv'\n",
    "pheno_merge_late_convert_name = path_name + pheno_file_name_late_convert\n",
    "\n",
    "# brain measures (left entorhinal thickavg):\n",
    "df_pheno_merge_late_convert, dict_roi_name, dict_dynamic_cov_name = load_info_from_file(pheno_merge_late_convert_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)\n",
    "\n",
    "# Calcuate the number of participants\n",
    "n_subject_late_convert = len(df_pheno_merge_late_convert)\n",
    "grm_late_convert = load_grm_info_from_file(n_subject_late_convert, filename = None)\n",
    "\n",
    "# load phenotype information and get average measures as the longitudinal measure to train/test\n",
    "total_measure_late_convert = []\n",
    "longit_measure_late_convert = []\n",
    "for roi, longit_roi in dict_roi_name.items():\n",
    "    total_measure_late_convert.append(df_pheno_merge_late_convert[longit_roi])\n",
    "\n",
    "longit_measure_late_convert = np.mean(total_measure_late_convert, axis = 0)\n",
    "longit_measure_late_convert = np.reshape(longit_measure_late_convert, [-1, 1], order = 'C')\n",
    "#print(longit_measure_late_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3379b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "# the number of measurements per each subject\n",
    "#subset_df = df[dict_roi_name[0]]\n",
    "count_measure_late_convert = []\n",
    "for roi, col_name in dict_roi_name.items():\n",
    "    # left and right\n",
    "    count_late_convert = df_pheno_merge_late_convert[col_name].notna().sum(axis = 1)\n",
    "    count_measure_late_convert.append(count_late_convert)\n",
    "measurement_late_convert = np.min(count_measure_late_convert, axis = 0)\n",
    "\n",
    "# get mapping matrix T to map from cross sectional to longitudinal\n",
    "T_late_convert = mapping_matrix_T(measurement_late_convert)\n",
    "print(len(T_late_convert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31acff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOE4\n"
     ]
    }
   ],
   "source": [
    "# do we need group info for cross validation from meausrement? genetic correlated?\n",
    "k = 0\n",
    "groups_late_convert = []\n",
    "for cnt_meas in measurement_late_convert:\n",
    "    for i in range(cnt_meas):\n",
    "        groups_late_convert.append(k)\n",
    "    k = k + 1\n",
    "groups_late_convert = np.array(groups_late_convert)\n",
    "\n",
    "# create subject level to represent scannner/location specific effects\n",
    "SubjID_late_convert = df_pheno_merge_late_convert[subject_ID_name]\n",
    "subject_level_late_convert = [ID[:subject_ID_slice_length] for ID in SubjID_late_convert]\n",
    "\n",
    "# load ranfom effect information\n",
    "longit_grm_late_convert, longit_C1_late_convert, longit_C2_late_convert, longit_block_diag_matrix_late_convert, longit_error_late_convert = load_random_effect_info_from_file(grm_late_convert, measurement_late_convert, subject_level_late_convert, T_late_convert)\n",
    "#print(longit_C1)\n",
    "#print(longit_C2)\n",
    "#print(longit_block_diag_matrix)\n",
    "longit_X_late_convert = load_covar_info_from_file(df_pheno_merge_late_convert, covar_stable_name_list, dict_dynamic_cov_name, measurement_late_convert, T_late_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1b7436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_early_est, var_g_early_est, var_t_early_est, var_c_early_est, var_e_early_est, h2_early_est, beta_early_est = ARLMM_late_converter(longit_measure_late_convert, longit_X_late_convert, grm_late_convert, measurement_late_convert, subject_level_late_convert, T_late_convert, stable_parameter)\n",
    "rho_early_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab62df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64661076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270bb3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0fd87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aliceyang/ADNI1plus2_Longitidinal_Cortical_Subcortical/Cortical_Thickness_SurfArea_Final/ADNI1plus2_cortical_Converter_final_sort.csv\n"
     ]
    }
   ],
   "source": [
    "pheno_convert_file = '/ADNI1plus2_cortical_Converter_final_sort.csv'\n",
    "pheno_merge_convert_name = path_name + pheno_convert_file\n",
    "print(pheno_merge_convert_name)\n",
    "df_pheno_merge_convert, dict_roi_name, dict_dynamic_cov_name = load_info_from_file(pheno_merge_convert_name, ROI_name_list, covar_dynamic_name_list, meausure_time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09ff1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the number of participants\n",
    "n_subject_convert = len(df_pheno_merge_convert)\n",
    "grm_convert = load_grm_info_from_file(n_subject_convert, filename = None)\n",
    "\n",
    "# load phenotype information and get average measures as the longitudinal measure to train/test\n",
    "total_measure_convert = []\n",
    "longit_measure_convert = []\n",
    "for roi, longit_roi in dict_roi_name.items():\n",
    "    total_measure_convert.append(df_pheno_merge_convert[longit_roi])\n",
    "\n",
    "longit_measure_convert = np.mean(total_measure_convert, axis = 0)\n",
    "longit_measure_convert = np.reshape(longit_measure_convert, [-1, 1], order = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ee9278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(longit_measure_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f01ae743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "# the number of measurements per each subject\n",
    "#subset_df = df[dict_roi_name[0]]\n",
    "count_measure_convert = []\n",
    "for roi, col_name in dict_roi_name.items():\n",
    "    # left and right\n",
    "    count_convert = df_pheno_merge_convert[col_name].notna().sum(axis = 1)\n",
    "    count_measure_convert.append(count_convert)\n",
    "measurement_convert = np.min(count_measure_convert, axis = 0)\n",
    "\n",
    "# get mapping matrix T to map from cross sectional to longitudinal\n",
    "T_convert = mapping_matrix_T(measurement_convert)\n",
    "print(len(T_convert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536be689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOE4\n"
     ]
    }
   ],
   "source": [
    "# do we need group info for cross validation from meausrement? genetic correlated?\n",
    "k = 0\n",
    "groups_convert = []\n",
    "for cnt_meas in measurement_convert:\n",
    "    for i in range(cnt_meas):\n",
    "        groups_convert.append(k)\n",
    "    k = k + 1\n",
    "groups_convert = np.array(groups_convert)\n",
    "\n",
    "# create subject level to represent scannner/location specific effects\n",
    "SubjID_convert = df_pheno_merge_convert[subject_ID_name]\n",
    "subject_level_convert = [ID[:subject_ID_slice_length] for ID in SubjID_convert]\n",
    "\n",
    "# load ranfom effect information\n",
    "longit_grm_convert, longit_C1_convert, longit_C2_convert, longit_block_diag_matrix_convert, longit_error_convert = load_random_effect_info_from_file(grm_convert, measurement_convert, subject_level_convert, T_convert)\n",
    "#print(longit_C1)\n",
    "#print(longit_C2)\n",
    "#print(longit_block_diag_matrix)\n",
    "longit_X_convert = load_covar_info_from_file(df_pheno_merge_convert, covar_stable_name_list, dict_dynamic_cov_name, measurement_convert, T_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b9ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  65.3  0.   1. ]\n",
      " [ 1.  66.4  0.   1. ]\n",
      " [ 1.  67.5  0.   1. ]\n",
      " ...\n",
      " [ 1.  74.4  1.   2. ]\n",
      " [ 1.  75.5  1.   2. ]\n",
      " [ 1.  76.5  1.   2. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.462564264618788"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(longit_X_convert)\n",
    "np.mean(longit_X_convert[:, 1])\n",
    "np.std(longit_X_convert[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fc62488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(longit_X_convert[:, 3] == 0).sum()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "41/56/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944b6e4",
   "metadata": {},
   "source": [
    "# Linear Regression/MMHE results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5063623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6198d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(longit_measure_convert, longit_X_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a7356b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f037db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16793109, -0.0133062 , -0.05034552, -0.01498589])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c262b816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const    1.122631e-95\n",
       "x1       3.422980e-18\n",
       "x2       2.275139e-02\n",
       "x3       3.088727e-01\n",
       "Name: P>|t|, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary2().tables[1]['P>|t|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d420de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d53689c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b4a9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation metrics\n",
    "mse_train_scores = []\n",
    "mae_train_scores = []\n",
    "r2_train_scores = []\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "mse_test_scores = []\n",
    "mae_test_scores = []\n",
    "r2_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33c35652",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(longit_X_convert):\n",
    "    X_train, X_test = longit_X_convert[train_index], longit_X_convert[test_index]\n",
    "    y_train, y_test = longit_measure_convert[train_index], longit_measure_convert[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    model = sm.OLS(y_train, X_train)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = results.predict(X_test)\n",
    "    y_train_pred = results.predict(X_train)\n",
    "    # Calculate evaluation metrics\n",
    "    mse_test_scores.append(mean_squared_error(y_test, y_pred))\n",
    "    mae_test_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "    r2_test_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    mse_train_scores.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mae_train_scores.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    r2_train_scores.append(r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "754ddb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2011042562507692\n",
      "0.16121302929603792\n",
      "0.21720106998598201\n",
      "0.20234420981601342\n",
      "0.16181837675590705\n",
      "0.19737593057135275\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(np.mean(mse_train_scores)))\n",
    "print(np.mean(mae_train_scores))\n",
    "print(np.mean(r2_train_scores))\n",
    "\n",
    "print(np.sqrt(np.mean(mse_test_scores)))\n",
    "print(np.mean(mae_test_scores))\n",
    "print(np.mean(r2_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df804b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db7407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
